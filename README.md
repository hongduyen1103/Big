# BÃO CÃO BÃ€I Táº¬P Lá»šN

# PHÃ‚N TÃCH Xá»¬ LÃ Dá»® LIá»†U Lá»šN

## SMART JOB MARKET INTELLIGENCE SYSTEM
## PHÃ‚N TÃCH & Dá»° ÄOÃN THá»Š TRÆ¯á»œNG LAO Äá»˜NG THÃ”NG MINH

**TRÆ¯á»œNG Äáº I Há»ŒC VINH**  
**VIá»†N Ká»¸ THUáº¬T VÃ€ CÃ”NG NGHá»†**  

**Lá»šP:** LT01 - **NHÃ“M:** 01  

Nghá»‡ An, 01/2026

---

## Lá»œI NÃ“I Äáº¦U

TrÆ°á»›c Ä‘Ã¢y, khi máº¡ng Internet cÃ²n chÆ°a phÃ¡t triá»ƒn, lÆ°á»£ng dá»¯ liá»‡u con ngÆ°á»i sinh ra khÃ¡ nhá» giá»t vÃ  thÆ°a thá»›t, nhÃ¬n chung, lÆ°á»£ng dá»¯ liá»‡u nÃ y váº«n náº±m trong kháº£ nÄƒng xá»­ lÃ½ cá»§a con ngÆ°á»i dÃ¹ báº±ng tay hay báº±ng mÃ¡y tÃ­nh. Tuy nhiÃªn trong ká»· nguyÃªn sá»‘, khi mÃ  sá»± bÃ¹ng ná»• cÃ´ng nghá»‡ truyá»n thÃ´ng Ä‘Ã£ dáº«n tá»›i sá»± bÃ¹ng ná»• dá»¯ liá»‡u ngÆ°á»i dÃ¹ng, lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c táº¡o ra vÃ´ cÃ¹ng lá»›n vÃ  Ä‘a dáº¡ng, Ä‘Ã²i há»i má»™t há»‡ thá»‘ng Ä‘á»§ máº¡nh Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  xá»­ lÃ½ nhá»¯ng dá»¯ liá»‡u Ä‘Ã³.

KhÃ¡i niá»‡m Big Data Ä‘á» cáº­p tá»›i dá»¯ liá»‡u lá»›n theo 3 khÃ­a canh khÃ¡c nhau, thá»© nháº¥t lÃ  tá»‘c Ä‘á»™ sinh dá»¯ liá»‡u (velocity), thá»© hai lÃ  lÆ°á»£ng dá»¯ liá»‡u (volume) vÃ  thá»© ba lÃ  Ä‘á»™ Ä‘a dáº¡ng (variety). LÆ°á»£ng dá»¯ liá»‡u nÃ y cÃ³ thá»ƒ Ä‘áº¿n tá»« nhiá»u nguá»“n khÃ¡c nhau nhÆ° cÃ¡c ná»n táº£ng truyá»n thÃ´ng Google, Facebook, Twitter, â€¦ hay thÃ´ng sá»‘ thu tháº­p tá»« cÃ¡c cáº£m biáº¿n, thiáº¿t bá»‹ IoT trong Ä‘á»i sá»‘ng, â€¦ VÃ  má»™t sá»± tháº­t ráº±ng doanh nghiá»‡p nÃ o cÃ³ thá»ƒ kiá»ƒm soÃ¡t vÃ  táº¡o ra tri thá»©c tá»« nhá»¯ng dá»¯ liá»‡u nÃ y sáº½ táº¡o ra má»™t tiá»m lá»±c ráº¥t lá»›n Ä‘á»ƒ cáº¡nh tranh vá»›i nhá»¯ng doanh nghiá»‡p khÃ¡c. CÃ³ thá»ƒ nÃ³i ráº±ng dá»¯ liá»‡u lÃ  sá»©c máº¡nh cá»§a ká»· nguyÃªn sá»‘ cÅ©ng khÃ´ng há» ngoa má»™t chÃºt nÃ o.

Äá»ƒ tiáº¿p cáº­n vá»›i lÄ©nh vá»±c nÃ y, nhÃ³m chÃºng em quyáº¿t Ä‘á»‹nh chá»n má»™t loáº¡i dá»¯ liá»‡u Ä‘á»§ lá»›n trong kháº£ nÄƒng Ä‘á»ƒ tiáº¿n hÃ nh tiáº¿n hÃ nh phÃ¢n tÃ­ch vÃ  lÆ°u trá»¯. ThÃ´ng tin tuyá»ƒn dá»¥ng viá»‡c lÃ m lÃ  má»™t trong nhá»¯ng thÃ´ng tin Ä‘Æ°á»£c nhiá»u ngÆ°á»i quan tÃ¢m, Ä‘áº·c biá»‡t lÃ  nhá»¯ng lao Ä‘á»™ng Ä‘ang cáº§n tÃ¬m viá»‡c lÃ m. Nhá»¯ng thÃ´ng tin nÃ y thÆ°á»ng xuáº¥t hiá»‡n á»Ÿ cÃ¡c nhÃ³m tuyá»ƒn dá»¥ng trÃªn máº¡ng xÃ£ há»™i vÃ  cÃ¡c trang web tuyá»ƒn dá»¥ng, trang tuyá»ƒn dá»¥ng riÃªng cá»§a cÃ´ng ty. Viá»‡c khai thÃ¡c Ä‘Æ°á»£c thÃ´ng tin nhu cáº§u tuyá»ƒn dá»¥ng cÃ³ thá»ƒ giÃºp cho ngÆ°á»i lao Ä‘á»™ng tÃ¬m Ä‘Æ°á»£c cÃ´ng viá»‡c phÃ¹ há»£p, cÃ¡c cÃ´ng ty cÃ³ thá»ƒ cÃ¢n nháº¯c Ä‘iá»u chá»‰nh, nhá»¯ng ngÆ°á»i Ä‘ang cÃ³ viá»‡c lÃ m cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c má»©c nÄƒng lá»±c cá»§a mÃ¬nh cÃ³ nháº­n Ä‘Æ°á»£c lá»£i Ã­ch phÃ¹ há»£p khi á»Ÿ cÃ´ng ty khÃ´ng hay cÅ©ng nhÆ° viá»‡c Ä‘iá»u chá»‰nh cÃ¡c chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o Ä‘á»ƒ táº¡o ra nguá»“n nhÃ¢n lá»±c phÃ¹ há»£p sau nÃ y. Äá»ƒ biáº¿t Ä‘Æ°á»£c thá»‹ trÆ°á»ng lao Ä‘á»™ng Ä‘ang cáº§n gÃ¬, má»™t giáº£i phÃ¡p Ä‘Æ¡n giáº£n mÃ  hiá»‡u quáº£ lÃ  thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡, thá»‘ng kÃª nhá»¯ng ká»¹ nÄƒng, kiáº¿n thá»©c Ä‘Æ°á»£c miÃªu táº£ trong cÃ¡c Ä‘Æ¡n tuyá»ƒn dá»¥ng cá»§a cÃ¡c cÃ´ng ty trÃªn cÃ¡c trang máº¡ng tÃ¬m viá»‡c lÃ m. CÃ¡c cÃ´ng Ä‘oáº¡n khi thá»±c hiá»‡n giáº£i phÃ¡p nÃ y cÆ¡ báº£n sáº½ bao gá»“m thu tháº­p dá»¯ liá»‡u, lá»c dá»¯ liá»‡u vÃ  biá»ƒu diá»…n, thá»‘ng kÃª dá»¯ liá»‡u.

Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i kháº£ nÄƒng má»Ÿ rá»™ng thu tháº­p dá»¯ liá»‡u tá»« nhiá»u trang web tuyá»ƒn dá»¥ng (TopCV, VietnamWorks, Vieclam24h, ViecOi), trong Ä‘Ã³ TopCV Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m nguá»“n dá»¯ liá»‡u chÃ­nh cho quÃ¡ trÃ¬nh triá»ƒn khai vÃ  demo há»‡ thá»‘ng.

BÃ i táº­p lá»›n cá»§a nhÃ³m chÃºng em bao gá»“m 3 ná»™i dung chÃ­nh:

1. Tá»•ng quan xÃ¢y dá»±ng há»‡ thá»‘ng
2. XÃ¢y dá»±ng chÆ°Æ¡ng trÃ¬nh vÃ  há»‡ thá»‘ng
3. Nháº­n xÃ©t, Ä‘Ã¡nh giÃ¡ vÃ  hÆ°á»›ng phÃ¡t triá»ƒn

Máº·c dÃ¹ Ä‘Ã£ cá»‘ gáº¯ng hoÃ n thiá»‡n sáº£n pháº©m nhÆ°ng khÃ´ng thá»ƒ trÃ¡nh khá»i nhá»¯ng thiáº¿u há»¥t vá» kiáº¿n thá»©c vÃ  sai sÃ³t trong kiá»ƒm thá»­. ChÃºng em ráº¥t mong nháº­n Ä‘Æ°á»£c nhá»¯ng nháº­n xÃ©t tháº³ng tháº¯n, chi tiáº¿t Ä‘áº¿n tá»« tháº§y TS. VÃµ Äá»©c Quang Ä‘á»ƒ tiáº¿p tá»¥c hoÃ n thiá»‡n hÆ¡n ná»¯a. Cuá»‘i cÃ¹ng, nhÃ³m chÃºng em xin Ä‘Æ°á»£c gá»­i lá»i cáº£m Æ¡n Ä‘áº¿n tháº§y TS. VÃµ Äá»©c Quang Ä‘Ã£ dáº«n chÃºng em trong suá»‘t quÃ¡ trÃ¬nh hoÃ n thiá»‡n BÃ i táº­p lá»›n. NhÃ³m chÃºng em xin chÃ¢n thÃ nh cáº£m Æ¡n tháº§y.

---

## CHÆ¯Æ NG 1: KIáº¾N TRÃšC VÃ€ THIáº¾T Káº¾ Há»† THá»NG

### 1.1. Tá»•ng quan há»‡ thá»‘ng

Há»‡ thá»‘ng Smart Job Market Intelligence System Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i kiáº¿n trÃºc phÃ¢n táº§ng hiá»‡n Ä‘áº¡i, tÃ­ch há»£p cÃ¡c cÃ´ng nghá»‡ Big Data tiÃªn tiáº¿n Ä‘á»ƒ xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u tuyá»ƒn dá»¥ng viá»‡c lÃ m. Há»‡ thá»‘ng bao gá»“m 4 thÃ nh pháº§n chÃ­nh vá»›i cÃ¡c chá»©c nÄƒng thu tháº­p, xá»­ lÃ½, lÆ°u trá»¯ vÃ  trá»±c quan hÃ³a dá»¯ liá»‡u.

#### 1.1.1. CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a há»‡ thá»‘ng

**Bá»™ pháº­n thu tháº­p dá»¯ liá»‡u (Data Ingestion Layer):**
- Sá»­ dá»¥ng BeautifulSoup4 vÃ  Scrapy/Selenium Ä‘á»ƒ crawl dá»¯ liá»‡u
- Thu tháº­p dá»¯ liá»‡u tá»« 4 trang web tuyá»ƒn dá»¥ng lá»›n nháº¥t Viá»‡t Nam
- Xá»­ lÃ½ dá»¯ liá»‡u real-time vá»›i lá»‹ch trÃ¬nh tá»± Ä‘á»™ng
- LÆ°u trá»¯ dá»¯ liá»‡u thÃ´ vÃ o há»‡ thá»‘ng streaming

**Bá»™ pháº­n lÆ°u trá»¯ (Storage Layer):**
- Hadoop Distributed File System (HDFS) cho lÆ°u trá»¯ phÃ¢n tÃ¡n
- PostgreSQL cho dá»¯ liá»‡u cÃ³ cáº¥u trÃºc
- Replication factor 2 Ä‘áº£m báº£o fault tolerance
- Kháº£ nÄƒng má»Ÿ rá»™ng theo nhu cáº§u

**Bá»™ pháº­n xá»­ lÃ½ dá»¯ liá»‡u (Processing Layer):**
- Apache Spark vá»›i MLlib cho machine learning
- Xá»­ lÃ½ batch vÃ  streaming data
- Feature engineering vÃ  data cleaning
- Triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n thÃ´ng minh

**Bá»™ pháº­n trá»±c quan hÃ³a (Presentation Layer):**
- Elasticsearch cho indexing vÃ  search
- Kibana cho dashboard vÃ  visualization
- Flask REST API cho external integration
- Web UI demo vá»›i user-friendly interface

### 1.2. Kiáº¿n trÃºc tá»•ng thá»ƒ cá»§a há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SMART JOB MARKET INTELLIGENCE                 â”‚
â”‚                           SYSTEM                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Data Sources  â”‚    â”‚  Data Ingestion â”‚    â”‚   Data Storage  â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚ â”‚
â”‚  â”‚  â€¢ TopCV        â”‚â”€â”€â”€â–¶â”‚  â€¢ Scrapy       â”‚â”€â”€â”€â–¶â”‚  â€¢ HDFS        â”‚ â”‚
â”‚  â”‚  â€¢ VietnamWorks â”‚    â”‚  â€¢ Selenium     â”‚    â”‚  â€¢ PostgreSQL  â”‚ â”‚
â”‚  â”‚  â€¢ Vieclam24h   â”‚    â”‚  â€¢ BeautifulSoupâ”‚    â”‚  â€¢ Kafka       â”‚ â”‚
â”‚  â”‚  â€¢ ViecOi       â”‚    â”‚  â€¢ Cron Jobs    â”‚    â”‚                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â”‚                       â”‚                       â”‚      â”‚
â”‚            â–¼                       â–¼                       â–¼      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Processing â”‚    â”‚   ML Analytics  â”‚    â”‚  Data Indexing  â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚ â”‚
â”‚  â”‚  â€¢ Apache Spark â”‚â”€â”€â”€â–¶â”‚  â€¢ Salary Pred  â”‚â”€â”€â”€â–¶â”‚  â€¢ Elasticsearchâ”‚ â”‚
â”‚  â”‚  â€¢ PySpark      â”‚    â”‚  â€¢ Job Classify â”‚    â”‚  â€¢ Kibana       â”‚ â”‚
â”‚  â”‚  â€¢ MLlib        â”‚    â”‚  â€¢ Trend Forecastâ”‚    â”‚  â€¢ Search API  â”‚ â”‚
â”‚  â”‚  â€¢ Batch/Stream â”‚    â”‚  â€¢ NLP Processingâ”‚    â”‚                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â”‚                       â”‚                       â”‚      â”‚
â”‚            â–¼                       â–¼                       â–¼      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Visualization  â”‚    â”‚      API        â”‚    â”‚   Web Demo UI   â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚ â”‚
â”‚  â”‚  â€¢ Kibana Dash  â”‚    â”‚  â€¢ Flask REST   â”‚    â”‚  â€¢ Career Guide â”‚ â”‚
â”‚  â”‚  â€¢ Real-time    â”‚    â”‚  â€¢ JSON API     â”‚    â”‚  â€¢ Job Matching â”‚ â”‚
â”‚  â”‚  â€¢ Interactive  â”‚    â”‚  â€¢ External Int â”‚    â”‚  â€¢ Salary Calc  â”‚ â”‚
â”‚  â”‚  â€¢ Heat Maps    â”‚    â”‚  â€¢ Third-party  â”‚    â”‚                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               INFRASTRUCTURE: VirtualBox VMs                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Master Node: 8 CPU, 16GB RAM, Hadoop/Spark/ES Master        â”‚
â”‚  â€¢ Worker1: 6 CPU, 12GB RAM, Data Node, Worker Node             â”‚
â”‚  â€¢ Worker2: 6 CPU, 12GB RAM, Data Node, Worker Node             â”‚
â”‚  â€¢ Network: 172.16.232.0/22, Bridged Adapter                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Luá»“ng dá»¯ liá»‡u chÃ­nh:**
1. **Thu tháº­p:** Scrapy/Selenium thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c trang tuyá»ƒn dá»¥ng
2. **LÆ°u trá»¯:** Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u vÃ o HDFS vÃ  PostgreSQL
3. **Xá»­ lÃ½:** Spark xá»­ lÃ½ batch/streaming vÃ  Ã¡p dá»¥ng ML models
4. **Index:** Elasticsearch index dá»¯ liá»‡u cho tÃ¬m kiáº¿m nhanh
5. **Trá»±c quan:** Kibana táº¡o dashboard, Flask API phá»¥c vá»¥ á»©ng dá»¥ng

### 1.3. Chi tiáº¿t thÃ nh pháº§n há»‡ thá»‘ng

#### 1.3.1. Data Ingestion vá»›i Scrapy/Selenium

Scrapy Ä‘Æ°á»£c chá»n lÃ m cÃ´ng cá»¥ crawl chÃ­nh vÃ¬:

- **Hiá»‡u suáº¥t cao** vá»›i asynchronous processing
- **Middleware linh hoáº¡t** Ä‘á»ƒ xá»­ lÃ½ JavaScript vÃ  authentication
- **Pipeline Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u ngay khi crawl**
- **Built-in support cho distributed crawling**

Selenium Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c trang web Ä‘á»™ng yÃªu cáº§u JavaScript rendering hoÃ n toÃ n.

**Cáº¥u trÃºc dá»¯ liá»‡u thu tháº­p:**
```json
{
  "job_id": "string",
  "title": "string",
  "company": "string",
  "location": "string",
  "salary": "string",
  "description": "string",
  "requirements": "string",
  "benefits": "string",
  "posted_date": "datetime",
  "source_url": "string",
  "skills": ["array"],
  "experience": "string"
}
```

#### 1.3.2. Hadoop Distributed File System (HDFS)

HDFS Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i:

| ThÃ´ng sá»‘ | GiÃ¡ trá»‹ | MÃ´ táº£ |
|----------|---------|-------|
| Block size | 128MB | Tá»‘i Æ°u cho big data |
| Replication factor | 2 | Fault tolerance cho 3-node cluster |
| DataNodes | 2 nodes | LÆ°u trá»¯ dá»¯ liá»‡u thá»±c táº¿ |
| NameNode HA | Secondary NameNode | Backup metadata |

**Cáº¥u trÃºc thÆ° má»¥c HDFS:**
```
/raw-data/           # Dá»¯ liá»‡u thÃ´ tá»« crawler
â”œâ”€â”€ topcv/           # Dá»¯ liá»‡u tá»« TopCV
â”œâ”€â”€ vietnamworks/    # Dá»¯ liá»‡u tá»« VietnamWorks
â”œâ”€â”€ vieclam24h/      # Dá»¯ liá»‡u tá»« Vieclam24h
â””â”€â”€ viecoi/          # Dá»¯ liá»‡u tá»« ViecOi

/processed-data/     # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”œâ”€â”€ cleaned/         # Dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch
â”œâ”€â”€ features/        # Features cho ML
â”œâ”€â”€ predictions/     # Káº¿t quáº£ dá»± Ä‘oÃ¡n
â””â”€â”€ analytics/       # Dá»¯ liá»‡u phÃ¢n tÃ­ch

/spark-data/         # Dá»¯ liá»‡u Spark
â”œâ”€â”€ events/          # Spark event logs
â”œâ”€â”€ warehouse/       # Spark metastore
â””â”€â”€ checkpoints/     # Streaming checkpoints
```

#### 1.3.3. Há»‡ sinh thÃ¡i Apache Spark

**Apache Spark** (gá»i táº¯t lÃ  Spark) lÃ  framework xá»­ lÃ½ dá»¯ liá»‡u lá»›n phÃ¢n tÃ¡n, cung cáº¥p kháº£ nÄƒng xá»­ lÃ½ song song (parallel processing) vá»›i hiá»‡u suáº¥t cao. Trong há»‡ thá»‘ng cá»§a chÃºng ta, Spark Ä‘Ã³ng vai trÃ² lÃ  engine xá»­ lÃ½ dá»¯ liá»‡u chÃ­nh.

**Cáº¥u trÃºc cluster Spark:**

| ThÃ nh pháº§n | Cáº¥u hÃ¬nh | Chá»©c nÄƒng |
|------------|----------|-----------|
| **NÃºt Master** (Master Node) | 8 CPU, 16GB RAM | Quáº£n lÃ½ tÃ i nguyÃªn, láº­p lá»‹ch tÃ¡c vá»¥ |
| **NÃºt Worker** (Worker Nodes) | 6 CPU, 12GB RAM má»—i nÃºt | Thá»±c thi tÃ¡c vá»¥ |
| **MLlib** | ThÆ° viá»‡n há»c mÃ¡y | Thuáº­t toÃ¡n vÃ  pipeline ML |
| **Spark SQL** | Xá»­ lÃ½ dá»¯ liá»‡u cÃ³ cáº¥u trÃºc | Truy váº¥n vÃ  phÃ¢n tÃ­ch |
| **Spark Streaming** | Xá»­ lÃ½ thá»i gian thá»±c | Dá»¯ liá»‡u streaming |

**Cáº¥u hÃ¬nh Spark:**
```properties
# Cáº¥u hÃ¬nh cho NÃºt Master
spark.master                    spark://master:7077
spark.executor.memory          4g          # Bá»™ nhá»› cho má»—i executor
spark.driver.memory            2g          # Bá»™ nhá»› cho driver
spark.serializer               KryoSerializer  # Serializer hiá»‡u suáº¥t cao
spark.sql.warehouse.dir        hdfs://master:9000/spark-warehouse
spark.es.nodes                 master       # Káº¿t ná»‘i Elasticsearch
spark.es.port                  9200

# Cáº¥u hÃ¬nh cho NÃºt Worker
spark.worker.cores             4           # Sá»‘ core CPU má»—i worker
spark.worker.memory            8g          # Bá»™ nhá»› má»—i worker
spark.worker.dir               /tmp/spark-work  # ThÆ° má»¥c lÃ m viá»‡c
```

**CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a Spark:**

1. **Spark Core**: Engine xá»­ lÃ½ cÆ¡ báº£n vá»›i RDD (Resilient Distributed Dataset)
2. **Spark SQL**: Xá»­ lÃ½ dá»¯ liá»‡u cÃ³ cáº¥u trÃºc vá»›i DataFrame API
3. **Spark Streaming**: Xá»­ lÃ½ dá»¯ liá»‡u real-time
4. **MLlib**: ThÆ° viá»‡n machine learning phÃ¢n tÃ¡n
5. **GraphX**: Xá»­ lÃ½ Ä‘á»“ thá»‹

#### 1.3.4. Cluster Elasticsearch

**Elasticsearch** (gá»i táº¯t lÃ  ES) lÃ  cÃ´ng cá»¥ tÃ¬m kiáº¿m vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u phÃ¢n tÃ¡n, cung cáº¥p kháº£ nÄƒng tÃ¬m kiáº¿m full-text, analytics thá»i gian thá»±c vÃ  kháº£ nÄƒng má»Ÿ rá»™ng cao. Trong há»‡ thá»‘ng, ES Ä‘Ã³ng vai trÃ² lÆ°u trá»¯ vÃ  tÃ¬m kiáº¿m dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½.

**Cáº¥u hÃ¬nh cluster:**

| ThÃ´ng sá»‘ | GiÃ¡ trá»‹ | Má»¥c Ä‘Ã­ch |
|----------|---------|----------|
| **Primary shards** (PhÃ¢n Ä‘oáº¡n chÃ­nh) | 5 | PhÃ¢n tÃ¡n dá»¯ liá»‡u ngang |
| **Replica shards** (PhÃ¢n Ä‘oáº¡n sao) | 1 | Äáº£m báº£o tÃ­nh kháº£ dá»¥ng |
| **Index templates** (Máº«u chá»‰ má»¥c) | Tá»± Ä‘á»™ng | Cáº¥u hÃ¬nh dá»¯ liá»‡u viá»‡c lÃ m |
| **Custom analyzers** (TrÃ¬nh phÃ¢n tÃ­ch) | Tiáº¿ng Viá»‡t | Tá»‘i Æ°u hÃ³a tÃ¬m kiáº¿m |
| **JVM Heap** (Bá»™ nhá»› heap) | 4GB má»—i nÃºt | Hiá»‡u suáº¥t xá»­ lÃ½ |

**Mapping chá»‰ má»¥c cho dá»¯ liá»‡u viá»‡c lÃ m:**
```json
{
  "mappings": {
    "properties": {
      "job_id": {
        "type": "keyword",
        "description": "MÃ£ Ä‘á»‹nh danh duy nháº¥t cá»§a viá»‡c lÃ m"
      },
      "title": {
        "type": "text",
        "analyzer": "vietnamese",
        "description": "TiÃªu Ä‘á» cÃ´ng viá»‡c vá»›i kháº£ nÄƒng tÃ¬m kiáº¿m tiáº¿ng Viá»‡t"
      },
      "company": {
        "type": "keyword",
        "description": "TÃªn cÃ´ng ty"
      },
      "location": {
        "type": "keyword",
        "description": "Äá»‹a Ä‘iá»ƒm lÃ m viá»‡c"
      },
      "salary_min": {
        "type": "integer",
        "description": "Má»©c lÆ°Æ¡ng tá»‘i thiá»ƒu (VNÄ)"
      },
      "salary_max": {
        "type": "integer",
        "description": "Má»©c lÆ°Æ¡ng tá»‘i Ä‘a (VNÄ)"
      },
      "salary_avg": {
        "type": "float",
        "description": "Má»©c lÆ°Æ¡ng trung bÃ¬nh dá»± Ä‘oÃ¡n"
      },
      "description": {
        "type": "text",
        "analyzer": "vietnamese",
        "description": "MÃ´ táº£ chi tiáº¿t cÃ´ng viá»‡c"
      },
      "requirements": {
        "type": "text",
        "analyzer": "vietnamese",
        "description": "YÃªu cáº§u cÃ´ng viá»‡c"
      },
      "skills": {
        "type": "keyword",
        "description": "Danh sÃ¡ch ká»¹ nÄƒng yÃªu cáº§u"
      },
      "experience_years": {
        "type": "integer",
        "description": "Sá»‘ nÄƒm kinh nghiá»‡m yÃªu cáº§u"
      },
      "posted_date": {
        "type": "date",
        "description": "NgÃ y Ä‘Äƒng tuyá»ƒn"
      },
      "predicted_salary": {
        "type": "float",
        "description": "Má»©c lÆ°Æ¡ng dá»± Ä‘oÃ¡n tá»« mÃ´ hÃ¬nh ML"
      },
      "job_category": {
        "type": "keyword",
        "description": "NgÃ nh nghá» phÃ¢n loáº¡i"
      },
      "company_size": {
        "type": "keyword",
        "description": "Quy mÃ´ cÃ´ng ty"
      }
    }
  }
}
```

**Kiáº¿n trÃºc cluster Elasticsearch:**
- **Master Node**: Quáº£n lÃ½ cluster, táº¡o chá»‰ má»¥c, phÃ¢n bá»• shards
- **Data Node**: LÆ°u trá»¯ dá»¯ liá»‡u, thá»±c hiá»‡n tÃ¬m kiáº¿m vÃ  aggregations
- **Discovery**: CÆ¡ cháº¿ tá»± Ä‘á»™ng phÃ¡t hiá»‡n cÃ¡c nÃºt trong cluster
- **Replication**: Sao chÃ©p dá»¯ liá»‡u Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh kháº£ dá»¥ng

#### 1.3.5. Kibana Dashboards

CÃ¡c dashboard chÃ­nh:

| Dashboard | Má»¥c Ä‘Ã­ch | Components |
|-----------|----------|------------|
| Overview | Tá»•ng quan thá»‹ trÆ°á»ng | KPIs, trends, heatmaps |
| Skills Analysis | PhÃ¢n tÃ­ch ká»¹ nÄƒng | Bar charts, word clouds |
| Salary Insights | ThÃ´ng tin lÆ°Æ¡ng | Box plots, scatter plots |
| Geographic View | PhÃ¢n bá»‘ Ä‘á»‹a lÃ½ | Maps, region charts |
| Trends Dashboard | Xu hÆ°á»›ng thá»i gian | Line charts, forecasting |
| ML Predictions | Káº¿t quáº£ dá»± Ä‘oÃ¡n | Accuracy metrics, predictions |

**Dashboard Features:**
- Real-time updates vá»›i auto-refresh
- Interactive filters vÃ  drill-down
- Export capabilities (PDF, PNG, CSV)
- Custom visualizations vá»›i Vega
- Alert system cho threshold breaches

### 1.4. CÃ¡c tÃ­nh nÄƒng cá»‘t lÃµi cá»§a há»‡ thá»‘ng

#### 1.4.1. PhÃ¢n tÃ­ch mÃ´ táº£ (Descriptive Analytics)

1. **Thá»‘ng kÃª ngÃ nh nghá» vÃ  ká»¹ nÄƒng hot nháº¥t:**
   - Top 20 ká»¹ nÄƒng Ä‘Æ°á»£c yÃªu cáº§u nhiá»u nháº¥t
   - PhÃ¢n bá»‘ theo ngÃ nh nghá» (IT, Marketing, Finance, etc.)
   - Trend analysis theo thá»i gian (thÃ¡ng/quÃ½)

2. **PhÃ¢n bá»‘ Ä‘á»‹a lÃ½ cÃ´ng viá»‡c:**
   - Heat map theo tá»‰nh/thÃ nh phá»‘
   - Bubble charts theo quy mÃ´ cÃ´ng ty
   - Geographic clustering

3. **Xu hÆ°á»›ng tuyá»ƒn dá»¥ng theo thá»i gian:**
   - Line charts theo thÃ¡ng/quÃ½/nÄƒm
   - Seasonality analysis
   - Growth rates

4. **Word cloud tá»« job descriptions:**
   - Trá»±c quan hÃ³a tá»« khÃ³a phá»• biáº¿n
   - TF-IDF weighting
   - Interactive filtering

#### 1.4.2. Dá»± Ä‘oÃ¡n thÃ´ng minh (Predictive Analytics)

**1. Salary Prediction Model:**
- **Thuáº­t toÃ¡n:** Random Forest Regression, Linear Regression, Gradient Boosting
- **Features:** skills, experience, location, company_size, industry
- **Target:** salary_range (min, max, average)
- **Metrics:** RMSE < 2M VND, RÂ² > 0.85, MAE < 1.5M VND

**2. Job Classification Model:**
- **Thuáº­t toÃ¡n:** Naive Bayes, SVM, Random Forest, BERT
- **Features:** job_title, description, requirements
- **Target:** job_category (IT, Marketing, Finance, etc.)
- **Metrics:** Accuracy > 89%, F1-score > 0.88

**3. Trend Forecasting Model:**
- **Thuáº­t toÃ¡n:** ARIMA, Exponential Smoothing, LSTM
- **Features:** time_series_data, seasonality, external factors
- **Target:** future_demand (3-6 thÃ¡ng)
- **Metrics:** MAPE < 12.5%, RMSE optimized

#### 1.4.3. Gá»£i Ã½ thÃ´ng minh (Prescriptive Analytics)

**1. Skill Gap Analysis:**
- XÃ¡c Ä‘á»‹nh ká»¹ nÄƒng Ä‘ang thiáº¿u trÃªn thá»‹ trÆ°á»ng
- So sÃ¡nh vá»›i ká»¹ nÄƒng cÃ¡ nhÃ¢n
- Äá» xuáº¥t roadmap há»c táº­p theo má»©c Ä‘á»™ Æ°u tiÃªn

**2. Career Path Suggestion:**
- PhÃ¢n tÃ­ch career trajectory
- Gá»£i Ã½ chuyá»ƒn Ä‘á»•i ngÃ nh nghá»
- Lá»i khuyÃªn vá» development plan

**3. Personalized Recommendations:**
- Job matching dá»±a trÃªn profile
- Salary negotiation insights
- Interview preparation tips

### 1.5. GiÃ¡ trá»‹ thá»±c tiá»…n cá»§a há»‡ thá»‘ng

Há»‡ thá»‘ng Smart Job Market Intelligence System khÃ´ng chá»‰ lÃ  cÃ´ng cá»¥ ká»¹ thuáº­t mÃ  cÃ²n mang láº¡i giÃ¡ trá»‹ thá»±c tiá»…n cao cho:

**NgÆ°á»i lao Ä‘á»™ng:**
- Hiá»ƒu rÃµ nhu cáº§u thá»‹ trÆ°á»ng vÃ  xu hÆ°á»›ng viá»‡c lÃ m
- Dá»± Ä‘oÃ¡n má»©c lÆ°Æ¡ng phÃ¹ há»£p vá»›i nÄƒng lá»±c vÃ  kinh nghiá»‡m
- Láº­p káº¿ hoáº¡ch phÃ¡t triá»ƒn ká»¹ nÄƒng hiá»‡u quáº£
- TÃ¬m Ä‘Æ°á»£c cÃ´ng viá»‡c phÃ¹ há»£p vá»›i Ä‘á»‹nh hÆ°á»›ng nghá» nghiá»‡p

**Doanh nghiá»‡p:**
- Chiáº¿n lÆ°á»£c tuyá»ƒn dá»¥ng hiá»‡u quáº£ dá»±a trÃªn data-driven insights
- XÃ¡c Ä‘á»‹nh má»©c lÆ°Æ¡ng cáº¡nh tranh trÃªn thá»‹ trÆ°á»ng
- Dá»± bÃ¡o nhu cáº§u nhÃ¢n lá»±c theo ngÃ nh vÃ  thá»i gian
- PhÃ¢n tÃ­ch Ä‘á»‘i thá»§ cáº¡nh tranh vÃ  benchmark

**NhÃ  quáº£n lÃ½ giÃ¡o dá»¥c:**
- Äiá»u chá»‰nh chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o theo nhu cáº§u thá»±c táº¿
- TÆ° váº¥n Ä‘á»‹nh hÆ°á»›ng nghá» nghiá»‡p cho sinh viÃªn
- Theo dÃµi káº¿t quáº£ employment cá»§a graduates
- XÃ¢y dá»±ng partnership vá»›i doanh nghiá»‡p

---

## CHÆ¯Æ NG 2: XÃ‚Y Dá»°NG CHÆ¯Æ NG TRÃŒNH VÃ€ Há»† THá»NG

### 2.1. Luá»“ng dá»¯ liá»‡u cá»§a há»‡ thá»‘ng

Luá»“ng dá»¯ liá»‡u cá»§a há»‡ thá»‘ng Smart Job Market Intelligence System gá»“m 8 quÃ¡ trÃ¬nh chÃ­nh:

```
1. Thu tháº­p dá»¯ liá»‡u â”€â”€â–º 2. Validation â”€â”€â–º 3. LÆ°u trá»¯ thÃ´ â”€â”€â–º 4. LÃ m sáº¡ch
     â”‚                        â”‚                        â”‚
     â–¼                        â–¼                        â–¼
5. Feature Engineering â”€â”€â–º 6. Machine Learning â”€â”€â–º 7. Indexing â”€â”€â–º 8. Visualization
```

#### 2.1.1. Chi tiáº¿t tá»«ng bÆ°á»›c xá»­ lÃ½

**BÆ°á»›c 1: Thu tháº­p dá»¯ liá»‡u (Data Collection)**
- Scrapy spiders crawl tá»« cÃ¡c trang tuyá»ƒn dá»¥ng
- Selenium xá»­ lÃ½ JavaScript rendering
- Cron jobs tá»± Ä‘á»™ng cháº¡y theo lá»‹ch trÃ¬nh
- Error handling vÃ  retry logic

**BÆ°á»›c 2: Validation vÃ  Cleaning**
- Schema validation
- Duplicate detection
- Data type conversion
- Missing value handling

**BÆ°á»›c 3: LÆ°u trá»¯ dá»¯ liá»‡u thÃ´**
- Raw data vÃ o HDFS
- Metadata vÃ o PostgreSQL
- Backup vÃ  replication

**BÆ°á»›c 4: Data Cleaning**
- Text normalization
- Outlier detection
- Standardization
- Quality assurance

### 2.2. Triá»ƒn khai háº¡ táº§ng há»‡ thá»‘ng

#### 2.2.1. MÃ´i trÆ°á»ng triá»ƒn khai

Há»‡ thá»‘ng Ä‘Æ°á»£c triá»ƒn khai trÃªn ná»n táº£ng áº£o hÃ³a VirtualBox vá»›i cáº¥u hÃ¬nh pháº§n cá»©ng vÃ  máº¡ng chi tiáº¿t:

**Cáº¥u hÃ¬nh mÃ¡y tráº¡m host:**
- **OS:** Ubuntu 22.04.5 LTS (Jammy Jellyfish)
- **CPU:** 48 threads (Intel Xeon processor)
- **RAM:** 128 GB DDR4
- **Storage:** 1.8TB NVMe SSD (cÃ²n trá»‘ng 1.7TB)
- **Network:** Intel X710 10GbE NIC (eno1np0 interface)
- **IP Address:** 172.16.232.16/22 (static)

**PhÃ¢n bá»• tÃ i nguyÃªn cho cluster 3 nodes:**

| VM Node | CPU Cores | RAM | Storage | IP Address | Hostname |
|---------|-----------|-----|---------|------------|----------|
| **Master** | 8 cores | 16GB | 80GB | 172.16.232.101 | master |
| **Worker1** | 6 cores | 12GB | 60GB | 172.16.232.102 | worker1 |
| **Worker2** | 6 cores | 12GB | 60GB | 172.16.232.103 | worker2 |
| **Tá»•ng cá»™ng** | 20 cores | 40GB | 200GB | - | - |

**SÆ¡ Ä‘á»“ máº¡ng vÃ  káº¿t ná»‘i:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Máº NG LAN Äáº I Há»ŒC VINH                     â”‚
â”‚           172.16.232.0/22 Subnet                    â”‚
â”‚           Gateway: 172.16.232.1                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚  â”‚ Ubuntu Host  â”‚  â—„â”€â”€ Bridged Adapter â”€â”€â”        â”‚
â”‚  â”‚ 172.16.232.16â”‚                        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚        â”‚
â”‚         â”‚                                 â”‚        â”‚
â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”‚
â”‚         â”‚   â”‚          VirtualBox VMs           â”‚  â”‚
â”‚         â”‚   â”‚                                   â”‚  â”‚
â”‚         â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚         â”‚   â”‚  â”‚ Master  â”‚  â”‚ Worker1 â”‚  â”‚ Worker2 â”‚ â”‚
â”‚         â”‚   â”‚  â”‚ .101    â”‚  â”‚ .102    â”‚  â”‚ .103    â”‚ â”‚
â”‚         â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚   â”‚                                       â”‚  â”‚
â”‚         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                               â”‚
â”‚         â””â”€ Sinh viÃªn trong LAN cÃ³ thá»ƒ truy cáº­p         â”‚
â”‚             - Hadoop NameNode: http://172.16.232.101:9870 â”‚
â”‚             - YARN ResourceManager: http://172.16.232.101:8088 â”‚
â”‚             - Spark Master: http://172.16.232.101:8080 â”‚
â”‚             - Elasticsearch: http://172.16.232.101:9200 â”‚
â”‚             - Kibana: http://172.16.232.101:5601 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ThÆ° má»¥c lÆ°u trá»¯ VMs:**
```
/home/[username]/Documents/Big-data/
â”œâ”€â”€ master/          # VM Master files
â”œâ”€â”€ worker1/         # VM Worker1 files
â””â”€â”€ worker2/         # VM Worker2 files
```

#### 2.2.2. Quy trÃ¬nh cÃ i Ä‘áº·t chi tiáº¿t

##### BÆ°á»›c 1: Chuáº©n bá»‹ VirtualBox

**1.1 Fix VirtualBox kernel module:**
```bash
# Cáº­p nháº­t há»‡ thá»‘ng
sudo apt update && sudo apt upgrade -y

# CÃ i Ä‘áº·t kernel headers
sudo apt install -y linux-headers-$(uname -r) dkms build-essential

# CÃ i Ä‘áº·t VirtualBox DKMS
sudo apt install --reinstall virtualbox-dkms

# Rebuild kernel module
sudo /sbin/vboxconfig

# Kiá»ƒm tra kernel module
lsmod | grep vbox
# Pháº£i tháº¥y: vboxdrv, vboxnetflt, vboxnetadp, vboxpci
```

**1.2 ThÃªm user vÃ o group vboxusers:**
```bash
# ThÃªm user hiá»‡n táº¡i vÃ o group
sudo usermod -aG vboxusers $USER

# Restart session hoáº·c cháº¡y
newgrp vboxusers

# Kiá»ƒm tra
groups | grep vboxusers
```

##### BÆ°á»›c 2: Táº£i Ubuntu Server ISO

**2.1 Download Ubuntu Server 22.04.5 LTS:**
```bash
# Táº¡o thÆ° má»¥c downloads
mkdir -p ~/Documents/ISOs
cd ~/Documents/ISOs

# Download ISO
wget https://releases.ubuntu.com/22.04/ubuntu-22.04.5-live-server-amd64.iso

# Kiá»ƒm tra file
ls -lh ubuntu-22.04.5-live-server-amd64.iso
# Size: ~2.6GB
```

##### BÆ°á»›c 3: Táº¡o vÃ  cáº¥u hÃ¬nh Master VM

**3.1 Khá»Ÿi táº¡o VM:**
1. Má»Ÿ VirtualBox â†’ New
2. Name: `bigdata-master`
3. Folder: `~/Documents/Big-data/master`
4. ISO: Chá»n file Ubuntu Server Ä‘Ã£ táº£i
5. Type: Linux â†’ Version: Ubuntu (64-bit)
6. Uncheck "Skip Unattended Installation"

**3.2 Cáº¥u hÃ¬nh pháº§n cá»©ng:**
1. Memory: 16384 MB (16GB)
2. Processors: 8 CPUs
3. Virtual Hard Disk: Create new â†’ VDI â†’ Dynamically allocated â†’ 80GB

**3.3 Cáº¥u hÃ¬nh máº¡ng:**
1. Settings â†’ Network â†’ Adapter 1
2. Attached to: Bridged Adapter
3. Name: eno1np0 (Intel X710 card)
4. Advanced â†’ Promiscuous Mode: Allow All

##### BÆ°á»›c 4: CÃ i Ä‘áº·t Ubuntu lÃªn Master VM

**4.1 Khá»Ÿi Ä‘á»™ng vÃ  cÃ i Ä‘áº·t:**
1. Start VM â†’ "Try or Install Ubuntu Server"
2. Language: English
3. Keyboard: English (US)
4. Network: DHCP (táº¡m thá»i)
5. Storage: Use entire disk â†’ VBOX HARDDISK
6. Profile: Hadoop User, Server name: master, Username: hadoop
7. SSH: Install OpenSSH server
8. Chá» cÃ i Ä‘áº·t hoÃ n táº¥t

**4.2 Cáº¥u hÃ¬nh sau cÃ i Ä‘áº·t:**
```bash
# ÄÄƒng nháº­p: hadoop/hadoop

# Cáº­p nháº­t há»‡ thá»‘ng
sudo apt update && sudo apt upgrade -y

# CÃ i Ä‘áº·t tools cáº§n thiáº¿t
sudo apt install -y nano wget curl net-tools htop openssh-server

# Táº¯t VM Ä‘á»ƒ chuáº©n bá»‹ snapshot
sudo shutdown -h now
```

**4.3 Táº¡o snapshot:**
- VirtualBox â†’ master VM â†’ Snapshots â†’ Take
- Name: `Fresh_Ubuntu_22.04`
- Description: Clean Ubuntu installation

##### BÆ°á»›c 5: Cáº¥u hÃ¬nh máº¡ng vÃ  hostname

**5.1 Äáº·t IP tÄ©nh cho Master:**
```bash
# Backup config cÅ©
sudo cp /etc/netplan/00-installer-config.yaml /etc/netplan/00-installer-config.yaml.bak

# Sá»­a netplan config
sudo nano /etc/netplan/00-installer-config.yaml
```

**Ná»™i dung file netplan:**
```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    enp0s3:
      dhcp4: no
      addresses:
        - 172.16.232.101/22
      routes:
        - to: default
          via: 172.16.232.1
      nameservers:
        addresses: [8.8.8.8, 8.4.4.4, 172.16.232.1]
        search: [vinhuni.edu.vn]
```

```bash
# Ãp dá»¥ng config
sudo netplan apply

# Äáº·t hostname
sudo hostnamectl set-hostname master

# Cáº­p nháº­t hosts file
sudo nano /etc/hosts
```

**ThÃªm vÃ o cuá»‘i file hosts:**
```
172.16.232.101  master
172.16.232.102  worker1
172.16.232.103  worker2
```

**5.2 Táº¡o thÆ° má»¥c há»‡ thá»‘ng:**
```bash
# Táº¡o thÆ° má»¥c cáº§n thiáº¿t
sudo mkdir -p /opt /data/hadoop /data/elasticsearch /data/kibana

# PhÃ¢n quyá»n cho hadoop user
sudo chown -R hadoop:hadoop /opt /data

# Restart vÃ  kiá»ƒm tra
sudo reboot
```

##### BÆ°á»›c 6: Clone Master thÃ nh Worker nodes

**6.1 Clone Worker1:**
1. VirtualBox â†’ master VM â†’ Machine â†’ Clone
2. Name: `bigdata-worker1`
3. Path: `~/Documents/Big-data/worker1`
4. MAC Address: Generate new
5. Clone type: Full clone
6. Settings â†’ System: RAM = 12288MB, CPU = 6

**6.2 Clone Worker2:**
1. TÆ°Æ¡ng tá»± Worker1 nhÆ°ng Name: `bigdata-worker2`
2. RAM = 12288MB, CPU = 6

##### BÆ°á»›c 7: Cáº¥u hÃ¬nh Worker nodes

**7.1 Cáº¥u hÃ¬nh Worker1:**
```bash
# Khá»Ÿi Ä‘á»™ng Worker1 VM
# ÄÄƒng nháº­p: hadoop/hadoop

# Äá»•i hostname
sudo hostnamectl set-hostname worker1

# Äá»•i IP
sudo nano /etc/netplan/00-installer-config.yaml
# Sá»­a addresses thÃ nh: 172.16.232.102/22

sudo netplan apply
sudo reboot
```

**7.2 Cáº¥u hÃ¬nh Worker2:**
```bash
# TÆ°Æ¡ng tá»± Worker1 nhÆ°ng hostname: worker2, IP: 172.16.232.103
```

##### BÆ°á»›c 8: CÃ i Ä‘áº·t Java vÃ  Python

**8.1 CÃ i Ä‘áº·t trÃªn táº¥t cáº£ VMs:**
```bash
# Java 11
sudo apt install -y openjdk-11-jdk

# Python 3
sudo apt install -y python3 python3-pip

# Kiá»ƒm tra
java -version
python3 --version
pip3 --version
```

**8.2 Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng:**
```bash
# ThÃªm vÃ o ~/.bashrc
nano ~/.bashrc

# ThÃªm cÃ¡c dÃ²ng sau:
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin

# Ãp dá»¥ng
source ~/.bashrc
```

##### BÆ°á»›c 9: Cáº¥u hÃ¬nh SSH passwordless

**9.1 Táº¡o SSH key trÃªn táº¥t cáº£ VMs:**
```bash
# Táº¡o key pair
ssh-keygen -t rsa -b 4096

# Copy public key vÃ o authorized_keys
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

# Copy key sang cÃ¡c node khÃ¡c
ssh-copy-id hadoop@master
ssh-copy-id hadoop@worker1
ssh-copy-id hadoop@worker2
```

**9.2 Test SSH:**
```bash
# Test tá»« Master
ssh hadoop@worker1 hostname  # Pháº£i tráº£ vá»: worker1
ssh hadoop@worker2 hostname  # Pháº£i tráº£ vá»: worker2
```

### 2.3. CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh Hadoop

#### 2.3.1. Download vÃ  cÃ i Ä‘áº·t Hadoop

**TrÃªn táº¥t cáº£ 3 VMs:**
```bash
# Download Hadoop 3.3.6
cd /tmp
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# Giáº£i nÃ©n
sudo tar -xzf hadoop-3.3.6.tar.gz -C /opt/
sudo mv /opt/hadoop-3.3.6 /opt/hadoop

# PhÃ¢n quyá»n
sudo chown -R hadoop:hadoop /opt/hadoop
```

#### 2.3.2. Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng Hadoop

**ThÃªm vÃ o ~/.bashrc trÃªn táº¥t cáº£ VMs:**
```bash
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HDFS_NAMENODE_USER=hadoop
export HDFS_DATANODE_USER=hadoop
export HDFS_SECONDARYNAMENODE_USER=hadoop
export YARN_RESOURCEMANAGER_USER=hadoop
export YARN_NODEMANAGER_USER=hadoop
```

#### 2.3.3. Cáº¥u hÃ¬nh Hadoop files

**hadoop-env.sh (táº¥t cáº£ VMs):**
```bash
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# ThÃªm:
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/opt/hadoop
export HADOOP_HEAPSIZE=4096
export HADOOP_NAMENODE_OPTS="-Xms4g -Xmx4g"
export HADOOP_DATANODE_OPTS="-Xms2g -Xmx2g"
```

**core-site.xml (táº¥t cáº£ VMs):**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/tmp/hadoop</value>
  </property>
  <property>
    <name>hadoop.http.staticuser.user</name>
    <value>hadoop</value>
  </property>
</configuration>
```

**hdfs-site.xml (Master):**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///data/hadoop/namenode</value>
  </property>
  <property>
    <name>dfs.namenode.http-address</name>
    <value>master:9870</value>
  </property>
  <property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
  </property>
</configuration>
```

**hdfs-site.xml (Worker1 & Worker2):**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///data/hadoop/datanode</value>
  </property>
  <property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
  </property>
</configuration>
```

**yarn-site.xml (táº¥t cáº£ VMs):**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>8192</value>
  </property>
</configuration>
```

**mapred-site.xml (táº¥t cáº£ VMs):**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

**workers file (chá»‰ Master):**
```
worker1
worker2
```

#### 2.3.4. Khá»Ÿi táº¡o vÃ  test Hadoop

**Táº¡o thÆ° má»¥c dá»¯ liá»‡u:**
```bash
# Master
sudo mkdir -p /data/hadoop/namenode /tmp/hadoop
sudo chown -R hadoop:hadoop /data/hadoop /tmp/hadoop

# Workers
sudo mkdir -p /data/hadoop/datanode /tmp/hadoop
sudo chown -R hadoop:hadoop /data/hadoop /tmp/hadoop
```

**Format vÃ  khá»Ÿi Ä‘á»™ng HDFS:**
```bash
# Chá»‰ trÃªn Master - format láº§n Ä‘áº§u
hdfs namenode -format

# Khá»Ÿi Ä‘á»™ng HDFS
start-dfs.sh

# Khá»Ÿi Ä‘á»™ng YARN
start-yarn.sh
```

**Test Hadoop:**
```bash
# Kiá»ƒm tra cluster
hdfs dfsadmin -report

# Test upload file
echo "Hello Big Data!" > test.txt
hdfs dfs -put test.txt /
hdfs dfs -ls /
hdfs dfs -cat /test.txt

# Cháº¡y MapReduce example
yarn jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 2 100
```

### 2.4. CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh Spark

#### 2.4.1. Download vÃ  cÃ i Ä‘áº·t Spark

**TrÃªn táº¥t cáº£ VMs:**
```bash
# Download Spark 3.5.0
cd /tmp
wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz

# Giáº£i nÃ©n
sudo tar -xzf spark-3.5.0-bin-hadoop3.tgz -C /opt/
sudo mv /opt/spark-3.5.0-bin-hadoop3 /opt/spark

# PhÃ¢n quyá»n
sudo chown -R hadoop:hadoop /opt/spark
```

#### 2.4.2. Cáº¥u hÃ¬nh Spark

**Biáº¿n mÃ´i trÆ°á»ng (táº¥t cáº£ VMs):**
```bash
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=/usr/bin/python3
export SPARK_LOCAL_IP=$(hostname -I | awk '{print $1}')
```

**spark-env.sh (táº¥t cáº£ VMs):**
```bash
cd $SPARK_HOME/conf
cp spark-env.sh.template spark-env.sh
nano spark-env.sh

# ThÃªm:
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export SPARK_MASTER_HOST=master
export SPARK_WORKER_CORES=4
export SPARK_WORKER_MEMORY=8g
export PYSPARK_PYTHON=/usr/bin/python3
```

**spark-defaults.conf (chá»‰ Master):**
```properties
spark.master                     spark://master:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://master:9000/spark-logs
spark.history.fs.logDirectory    hdfs://master:9000/spark-logs
spark.executor.memory            4g
spark.driver.memory              2g
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.sql.warehouse.dir          hdfs://master:9000/spark-warehouse
spark.es.nodes                   master
spark.es.port                    9200
```

**workers file (chá»‰ Master):**
```
worker1
worker2
```

**Táº¡o thÆ° má»¥c Spark:**
```bash
# Táº¥t cáº£ VMs
mkdir -p /tmp/spark-events
chmod 777 /tmp/spark-events

# Master
hdfs dfs -mkdir -p /spark-logs /spark-warehouse
hdfs dfs -chmod 777 /spark-logs /spark-warehouse
```

#### 2.4.3. Khá»Ÿi Ä‘á»™ng vÃ  test Spark

**Khá»Ÿi Ä‘á»™ng Spark cluster:**
```bash
# Master
$SPARK_HOME/sbin/start-all.sh

# Kiá»ƒm tra
jps  # Pháº£i tháº¥y: Master
```

**Workers:**
```bash
jps  # Pháº£i tháº¥y: Worker
```

**Test Spark:**
```bash
# Spark Shell
spark-shell --master spark://master:7077
scala> val data = 1 to 1000
scala> val distData = sc.parallelize(data)
scala> distData.filter(_ < 10).collect()

# PySpark
pyspark --master spark://master:7077
>>> data = range(1, 1001)
>>> dist_data = sc.parallelize(data)
>>> dist_data.filter(lambda x: x < 10).collect()
```

### 2.5. CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh Elasticsearch

#### 2.5.1. Download vÃ  cÃ i Ä‘áº·t Elasticsearch

**TrÃªn táº¥t cáº£ VMs:**
```bash
# Download Elasticsearch 8.11.4
cd /tmp
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.11.4-linux-x86_64.tar.gz

# Giáº£i nÃ©n
sudo tar -xzf elasticsearch-8.11.4-linux-x86_64.tar.gz -C /opt/
sudo mv /opt/elasticsearch-8.11.4 /opt/elasticsearch

# PhÃ¢n quyá»n
sudo chown -R hadoop:hadoop /opt/elasticsearch
```

#### 2.5.2. Cáº¥u hÃ¬nh system limits

**TrÃªn táº¥t cáº£ VMs:**
```bash
# ThÃªm vÃ o /etc/security/limits.conf
sudo nano /etc/security/limits.conf
# ThÃªm:
hadoop soft nofile 65536
hadoop hard nofile 65536
hadoop soft memlock unlimited
hadoop hard memlock unlimited

# Cáº¥u hÃ¬nh sysctl
sudo nano /etc/sysctl.conf
# ThÃªm:
vm.max_map_count=262144

# Ãp dá»¥ng
sudo sysctl -p
```

#### 2.5.3. Cáº¥u hÃ¬nh Elasticsearch

**elasticsearch.yml (Master):**
```yaml
cluster.name: bigdata-cluster
node.name: es-master
node.roles: [master, data]
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300
discovery.seed_hosts: ["master", "worker1", "worker2"]
cluster.initial_master_nodes: ["es-master"]
xpack.security.enabled: true
xpack.security.enrollment.enabled: false
path.data: /data/elasticsearch
path.logs: /data/elasticsearch/logs
bootstrap.memory_lock: false
```

**elasticsearch.yml (Worker1 & Worker2):**
```yaml
cluster.name: bigdata-cluster
node.name: es-worker1  # es-worker2 cho Worker2
node.roles: [data]
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300
discovery.seed_hosts: ["master", "worker1", "worker2"]
xpack.security.enabled: false
path.data: /data/elasticsearch
path.logs: /data/elasticsearch/logs
bootstrap.memory_lock: false
```

**JVM options:**
```bash
mkdir -p /opt/elasticsearch/config/jvm.options.d
nano /opt/elasticsearch/config/jvm.options.d/custom.options
# ThÃªm:
-Xms4g
-Xmx4g
```

**Táº¡o thÆ° má»¥c dá»¯ liá»‡u:**
```bash
sudo mkdir -p /data/elasticsearch
sudo chown -R hadoop:hadoop /data/elasticsearch
```

#### 2.5.4. Khá»Ÿi Ä‘á»™ng vÃ  test Elasticsearch

**Khá»Ÿi Ä‘á»™ng cluster:**
```bash
# Táº¥t cáº£ VMs
cd /opt/elasticsearch
nohup bin/elasticsearch > /dev/null 2>&1 &
```

**Test cluster:**
```bash
# Kiá»ƒm tra health
curl http://localhost:9200/_cluster/health?pretty

# Kiá»ƒm tra nodes
curl http://localhost:9200/_cat/nodes?v

# Test táº¡o index
curl -X PUT "http://localhost:9200/test-index"
curl -X POST "http://localhost:9200/test-index/_doc/1" \
  -H 'Content-Type: application/json' \
  -d '{"message": "Hello Elasticsearch!", "timestamp": "2025-01-08"}'
```

### 2.6. CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh Kibana

#### 2.6.1. Download vÃ  cÃ i Ä‘áº·t Kibana

**Chá»‰ trÃªn Master:**
```bash
# Download Kibana 8.11.4
cd /tmp
wget https://artifacts.elastic.co/downloads/kibana/kibana-8.11.4-linux-x86_64.tar.gz

# Giáº£i nÃ©n
sudo tar -xzf kibana-8.11.4-linux-x86_64.tar.gz -C /opt/
sudo mv /opt/kibana-8.11.4 /opt/kibana

# PhÃ¢n quyá»n
sudo chown -R hadoop:hadoop /opt/kibana
```

#### 2.6.2. Cáº¥u hÃ¬nh Kibana

**kibana.yml:**
```yaml
server.host: "0.0.0.0"
server.port: 5601
server.name: "kibana-master"
elasticsearch.hosts: ["http://master:9200"]
logging:
  appenders:
    file:
      type: file
      fileName: /data/kibana/kibana.log
      layout:
        type: json
  root:
    appenders:
      - default
      - file
    level: info
```

**Táº¡o thÆ° má»¥c logs:**
```bash
sudo mkdir -p /data/kibana
sudo chown -R hadoop:hadoop /data/kibana
```

#### 2.6.3. Khá»Ÿi Ä‘á»™ng vÃ  test Kibana

**Khá»Ÿi Ä‘á»™ng Kibana:**
```bash
cd /opt/kibana
nohup bin/kibana > /dev/null 2>&1 &
```

**Test truy cáº­p:**
- URL: http://172.16.232.101:5601
- Táº¡o Data View cho test-index
- Táº¡o visualizations cÆ¡ báº£n

### 2.7. CÃ i Ä‘áº·t Python packages vÃ  á»©ng dá»¥ng

#### 2.7.1. CÃ i Ä‘áº·t Python packages

**TrÃªn táº¥t cáº£ VMs:**
```bash
pip3 install beautifulsoup4==4.12.2
pip3 install requests
pip3 install pyspark==3.5.0
pip3 install elasticsearch==8.11.1
pip3 install hdfs3
pip3 install pandas
pip3 install scikit-learn
pip3 install flask
pip3 install flask-cors
```

#### 2.7.2. Triá»ƒn khai á»©ng dá»¥ng crawler

**Táº¡o thÆ° má»¥c á»©ng dá»¥ng (Master):**
```bash
mkdir -p /scripts
cd /scripts

# Táº¡o file crawler.py
nano crawler.py
```

**Ná»™i dung crawler.py:**
```python
#!/usr/bin/env python3
"""
Job Market Data Crawler
Thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c trang tuyá»ƒn dá»¥ng Viá»‡t Nam
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import sys
import os

class JobMarketCrawler:
    def __init__(self):
        self.sources = {
            'topcv': {
                'url': 'https://www.topcv.vn/tim-viec-lam-it-phan-mem',
                'headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            }
        }
        self.data_dir = '/data/jobs'
        os.makedirs(self.data_dir, exist_ok=True)

    def crawl_topcv(self, max_pages=5):
        """Crawl dá»¯ liá»‡u tá»« TopCV"""
        jobs = []

        for page in range(1, max_pages + 1):
            try:
                url = f"{self.sources['topcv']['url']}?page={page}"
                response = requests.get(url, headers=self.sources['topcv']['headers'])
                soup = BeautifulSoup(response.content, 'html.parser')

                job_cards = soup.find_all('div', class_='job-item')

                for card in job_cards:
                    job_data = self.extract_topcv_job(card)
                    if job_data:
                        jobs.append(job_data)

                print(f"ÄÃ£ crawl {len(job_cards)} jobs tá»« trang {page}")
                time.sleep(2)  # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block

            except Exception as e:
                print(f"Lá»—i khi crawl trang {page}: {e}")
                continue

        return jobs

    def extract_topcv_job(self, job_card):
        """Extract thÃ´ng tin job tá»« TopCV card"""
        try:
            title_elem = job_card.find('h3', class_='title')
            company_elem = job_card.find('a', class_='company')
            salary_elem = job_card.find('div', class_='salary')
            location_elem = job_card.find('div', class_='location')

            if not title_elem or not company_elem:
                return None

            return {
                'job_id': f"topcv_{int(time.time())}_{hash(str(title_elem.text))}",
                'title': title_elem.text.strip(),
                'company': company_elem.text.strip(),
                'salary': salary_elem.text.strip() if salary_elem else 'ThÆ°Æ¡ng lÆ°á»£ng',
                'location': location_elem.text.strip() if location_elem else 'Unknown',
                'description': '',
                'requirements': '',
                'benefits': '',
                'posted_date': datetime.now().isoformat(),
                'source_url': 'https://www.topcv.vn',
                'source': 'topcv',
                'crawled_at': datetime.now().isoformat()
            }
        except Exception as e:
            print(f"Lá»—i extract job: {e}")
            return None

    def save_to_json(self, jobs, filename=None):
        """LÆ°u dá»¯ liá»‡u vÃ o file JSON"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{self.data_dir}/jobs_{timestamp}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)

        print(f"ÄÃ£ lÆ°u {len(jobs)} jobs vÃ o {filename}")
        return filename

    def run(self, source='topcv', max_pages=5):
        """Cháº¡y crawler"""
        print(f"ğŸš€ Báº¯t Ä‘áº§u crawl dá»¯ liá»‡u tá»« {source}")
        print(f"ğŸ“„ Sá»‘ trang tá»‘i Ä‘a: {max_pages}")

        if source == 'topcv':
            jobs = self.crawl_topcv(max_pages)
        else:
            print(f"Source {source} chÆ°a Ä‘Æ°á»£c há»— trá»£")
            return

        if jobs:
            filename = self.save_to_json(jobs)
            print(f"âœ… HoÃ n thÃ nh! ÄÃ£ thu tháº­p {len(jobs)} jobs")
            return filename
        else:
            print("âŒ KhÃ´ng thu tháº­p Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o")
            return None

if __name__ == "__main__":
    crawler = JobMarketCrawler()

    # Cháº¡y vá»›i tham sá»‘ tá»« command line
    source = sys.argv[1] if len(sys.argv) > 1 else 'topcv'
    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 3

    crawler.run(source, max_pages)
```

#### 2.7.3. Triá»ƒn khai á»©ng dá»¥ng xá»­ lÃ½ dá»¯ liá»‡u

**Táº¡o file spark_processor.py:**
```python
#!/usr/bin/env python3
"""
Spark Job Data Processor
Xá»­ lÃ½ dá»¯ liá»‡u job market vá»›i Spark vÃ  ML
"""

import sys
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator
import json

class JobDataProcessor:
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("JobMarketProcessor") \
            .config("spark.es.nodes", "master") \
            .config("spark.es.port", "9200") \
            .getOrCreate()

        self.models = {}

    def load_data_from_hdfs(self, hdfs_path):
        """Load dá»¯ liá»‡u tá»« HDFS"""
        try:
            df = self.spark.read.json(hdfs_path)
            print(f"âœ… ÄÃ£ load {df.count()} records tá»« HDFS")
            return df
        except Exception as e:
            print(f"âŒ Lá»—i load data: {e}")
            return None

    def clean_data(self, df):
        """LÃ m sáº¡ch dá»¯ liá»‡u"""
        # Loáº¡i bá» records null
        df_clean = df.dropna(subset=['title', 'company'])

        # Chuáº©n hÃ³a text
        df_clean = df_clean.withColumn('title_clean',
            regexp_replace('title', '[^a-zA-Z0-9\sÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘Ã€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä]', ''))

        # Parse salary
        df_clean = df_clean.withColumn('salary_min',
            when(col('salary').contains('-'), split(col('salary'), '-')[0])
            .otherwise('0'))

        df_clean = df_clean.withColumn('salary_max',
            when(col('salary').contains('-'), split(col('salary'), '-')[1])
            .otherwise(col('salary_min')))

        # Convert to numeric
        df_clean = df_clean.withColumn('salary_min', regexp_replace('salary_min', '[^0-9]', '').cast('int'))
        df_clean = df_clean.withColumn('salary_max', regexp_replace('salary_max', '[^0-9]', '').cast('int'))

        print(f"âœ… ÄÃ£ lÃ m sáº¡ch dá»¯ liá»‡u: {df_clean.count()} records")
        return df_clean

    def feature_engineering(self, df):
        """Táº¡o features cho ML"""
        # Index categorical variables
        indexers = [
            StringIndexer(inputCol='location', outputCol='location_index'),
            StringIndexer(inputCol='company', outputCol='company_index')
        ]

        for indexer in indexers:
            model = indexer.fit(df)
            df = model.transform(df)
            self.models[indexer.getOutputCol()] = model

        # Create feature vector
        assembler = VectorAssembler(
            inputCols=['location_index', 'company_index'],
            outputCol='features'
        )

        df_featured = assembler.fit(df).transform(df)
        self.models['assembler'] = assembler

        print("âœ… ÄÃ£ táº¡o features cho ML")
        return df_featured

    def train_salary_prediction_model(self, df):
        """Train model dá»± Ä‘oÃ¡n lÆ°Æ¡ng"""
        # Filter data cÃ³ salary
        df_salary = df.filter(col('salary_min').isNotNull())

        # Split data
        train_data, test_data = df_salary.randomSplit([0.8, 0.2], seed=42)

        # Train model
        rf = RandomForestRegressor(
            featuresCol='features',
            labelCol='salary_min',
            numTrees=100,
            maxDepth=10
        )

        model = rf.fit(train_data)
        self.models['salary_predictor'] = model

        # Evaluate
        predictions = model.transform(test_data)
        evaluator = RegressionEvaluator(
            labelCol='salary_min',
            predictionCol='prediction',
            metricName='rmse'
        )

        rmse = evaluator.evaluate(predictions)
        print(".2f"
        # Save model
        model.write().overwrite().save('/models/salary_predictor')
        print("âœ… ÄÃ£ lÆ°u model dá»± Ä‘oÃ¡n lÆ°Æ¡ng")

        return model

    def train_job_classification_model(self, df):
        """Train model phÃ¢n loáº¡i job"""
        # Táº¡o target variable tá»« title
        df_classified = df.withColumn('job_category',
            when(col('title').contains('Data'), 'Data Science')
            .when(col('title').contains('DevOps'), 'DevOps')
            .when(col('title').contains('Frontend'), 'Frontend')
            .when(col('title').contains('Backend'), 'Backend')
            .otherwise('Other')
        )

        # Index target
        indexer = StringIndexer(inputCol='job_category', outputCol='label')
        df_classified = indexer.fit(df_classified).transform(df_classified)
        self.models['job_category_indexer'] = indexer

        # Split data
        train_data, test_data = df_classified.randomSplit([0.8, 0.2], seed=42)

        # Train model
        rf = RandomForestClassifier(
            featuresCol='features',
            labelCol='label',
            numTrees=50,
            maxDepth=8
        )

        model = rf.fit(train_data)
        self.models['job_classifier'] = model

        # Evaluate
        predictions = model.transform(test_data)
        evaluator = MulticlassClassificationEvaluator(
            labelCol='label',
            predictionCol='prediction',
            metricName='accuracy'
        )

        accuracy = evaluator.evaluate(predictions)
        print(".2f"
        # Save model
        model.write().overwrite().save('/models/job_classifier')
        print("âœ… ÄÃ£ lÆ°u model phÃ¢n loáº¡i job")

        return model

    def save_to_elasticsearch(self, df, index_name='jobs'):
        """LÆ°u dá»¯ liá»‡u vÃ o Elasticsearch"""
        try:
            df.write \
                .format("org.elasticsearch.spark.sql") \
                .option("es.nodes", "master") \
                .option("es.port", "9200") \
                .option("es.resource", index_name) \
                .mode("overwrite") \
                .save()

            print(f"âœ… ÄÃ£ lÆ°u {df.count()} records vÃ o Elasticsearch index: {index_name}")
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u Elasticsearch: {e}")

    def run_pipeline(self, input_path):
        """Cháº¡y toÃ n bá»™ pipeline xá»­ lÃ½"""
        print("ğŸš€ Báº¯t Ä‘áº§u pipeline xá»­ lÃ½ dá»¯ liá»‡u")

        # Load data
        df = self.load_data_from_hdfs(input_path)
        if df is None:
            return

        # Clean data
        df_clean = self.clean_data(df)

        # Feature engineering
        df_featured = self.feature_engineering(df_clean)

        # Train models
        salary_model = self.train_salary_prediction_model(df_featured)
        job_model = self.train_job_classification_model(df_featured)

        # Add predictions to data
        df_with_predictions = salary_model.transform(df_featured)
        df_with_predictions = job_model.transform(df_with_predictions)

        # Save to Elasticsearch
        self.save_to_elasticsearch(df_with_predictions, 'processed_jobs')

        print("âœ… HoÃ n thÃ nh pipeline xá»­ lÃ½ dá»¯ liá»‡u")

if __name__ == "__main__":
    processor = JobDataProcessor()

    # Input path tá»« command line hoáº·c default
    input_path = sys.argv[1] if len(sys.argv) > 1 else 'hdfs://master:9000/raw-data/topcv/jobs_*.json'

    processor.run_pipeline(input_path)
```

### 2.8. Triá»ƒn khai Flask API

**Táº¡o file app.py trÃªn Master:**
```python
#!/usr/bin/env python3
"""
Flask REST API cho Job Market Intelligence System
"""

from flask import Flask, jsonify, request
from flask_cors import CORS
from elasticsearch import Elasticsearch
import json
from datetime import datetime

app = Flask(__name__)
CORS(app)

# Káº¿t ná»‘i Elasticsearch
es = Elasticsearch(['http://master:9200'])

@app.route('/api/jobs', methods=['GET'])
def get_jobs():
    """Láº¥y danh sÃ¡ch jobs vá»›i filter"""
    try:
        # Parameters
        page = int(request.args.get('page', 1))
        size = int(request.args.get('size', 20))
        search = request.args.get('search', '')
        location = request.args.get('location', '')
        min_salary = request.args.get('min_salary', 0)

        # Build query
        query = {
            "bool": {
                "must": []
            }
        }

        if search:
            query["bool"]["must"].append({
                "multi_match": {
                    "query": search,
                    "fields": ["title", "description", "requirements"]
                }
            })

        if location:
            query["bool"]["must"].append({
                "match": {"location": location}
            })

        if min_salary:
            query["bool"]["must"].append({
                "range": {"salary_min": {"gte": int(min_salary)}}
            })

        # Search
        result = es.search(
            index='processed_jobs',
            body={
                "query": query,
                "from": (page - 1) * size,
                "size": size,
                "sort": [{"posted_date": {"order": "desc"}}]
            }
        )

        jobs = []
        for hit in result['hits']['hits']:
            job = hit['_source']
            job['id'] = hit['_id']
            job['score'] = hit['_score']
            jobs.append(job)

        return jsonify({
            'success': True,
            'data': jobs,
            'total': result['hits']['total']['value'],
            'page': page,
            'size': size
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/jobs/<job_id>', methods=['GET'])
def get_job_detail(job_id):
    """Láº¥y chi tiáº¿t job"""
    try:
        result = es.get(index='processed_jobs', id=job_id)
        job = result['_source']
        job['id'] = result['_id']

        return jsonify({
            'success': True,
            'data': job
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 404

@app.route('/api/predict-salary', methods=['POST'])
def predict_salary():
    """API dá»± Ä‘oÃ¡n lÆ°Æ¡ng"""
    try:
        data = request.json

        # Giáº£ láº­p prediction (thá»±c táº¿ sáº½ dÃ¹ng trained model)
        base_salary = 15000000  # 15 triá»‡u base

        # Factors affecting salary
        experience_multiplier = min(data.get('experience_years', 0) * 0.1 + 1, 2.0)
        skill_multiplier = min(len(data.get('skills', [])) * 0.05 + 1, 1.5)

        predicted_salary = base_salary * experience_multiplier * skill_multiplier

        return jsonify({
            'success': True,
            'prediction': {
                'salary_min': int(predicted_salary * 0.8),
                'salary_max': int(predicted_salary * 1.2),
                'confidence': 0.85
            }
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/skill-demand', methods=['GET'])
def get_skill_demand():
    """Láº¥y top skills Ä‘Æ°á»£c yÃªu cáº§u"""
    try:
        # Aggregation query
        result = es.search(
            index='processed_jobs',
            body={
                "size": 0,
                "aggs": {
                    "skills_terms": {
                        "terms": {
                            "field": "skills.keyword",
                            "size": 20
                        }
                    }
                }
            }
        )

        skills = []
        for bucket in result['aggregations']['skills_terms']['buckets']:
            skills.append({
                'skill': bucket['key'],
                'count': bucket['doc_count']
            })

        return jsonify({
            'success': True,
            'data': skills
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/trends', methods=['GET'])
def get_trends():
    """Láº¥y xu hÆ°á»›ng tuyá»ƒn dá»¥ng"""
    try:
        # Date histogram aggregation
        result = es.search(
            index='processed_jobs',
            body={
                "size": 0,
                "aggs": {
                    "jobs_over_time": {
                        "date_histogram": {
                            "field": "posted_date",
                            "calendar_interval": "month",
                            "format": "yyyy-MM"
                        }
                    }
                }
            }
        )

        trends = []
        for bucket in result['aggregations']['jobs_over_time']['buckets']:
            trends.append({
                'period': bucket['key_as_string'],
                'count': bucket['doc_count']
            })

        return jsonify({
            'success': True,
            'data': trends
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'services': {
            'elasticsearch': es.ping(),
            'api': True
        }
    })

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5000, debug=False)
```

### 2.9. Test há»‡ thá»‘ng hoÃ n chá»‰nh

#### 2.9.1. Test data pipeline

**Cháº¡y crawler:**
```bash
cd /scripts
python3 crawler.py topcv 3
```

**Upload dá»¯ liá»‡u lÃªn HDFS:**
```bash
hdfs dfs -mkdir -p /raw-data/topcv
hdfs dfs -put /data/jobs/jobs_*.json /raw-data/topcv/
```

**Cháº¡y Spark processor:**
```bash
python3 spark_processor.py
```

**Kiá»ƒm tra dá»¯ liá»‡u trong Elasticsearch:**
```bash
curl "http://master:9200/processed_jobs/_count?pretty"
curl "http://master:9200/processed_jobs/_search?size=5&pretty"
```

#### 2.9.2. Test API endpoints

**Khá»Ÿi Ä‘á»™ng Flask API:**
```bash
cd /scripts
python3 app.py &
```

**Test cÃ¡c endpoints:**
```bash
# Health check
curl http://master:5000/health

# Get jobs
curl "http://master:5000/api/jobs?page=1&size=10"

# Get skill demand
curl "http://master:5000/api/skill-demand"

# Get trends
curl "http://master:5000/api/trends"

# Predict salary
curl -X POST "http://master:5000/api/predict-salary" \
  -H "Content-Type: application/json" \
  -d '{"experience_years": 3, "skills": ["Python", "SQL", "Machine Learning"]}'
```

#### 2.9.3. Test Kibana dashboards

**Truy cáº­p Kibana:**
- URL: http://172.16.232.101:5601
- Táº¡o Data View cho processed_jobs index
- Táº¡o cÃ¡c visualizations:
  - Job postings over time (Line chart)
  - Top companies by job count (Bar chart)
  - Salary distribution (Histogram)
  - Geographic distribution (Map)

---

## CHÆ¯Æ NG 3: NHáº¬N XÃ‰T, ÄÃNH GIÃ VÃ€ HÆ¯á»šNG PHÃT TRIá»‚N

### 3.1. Nháº­n xÃ©t vÃ  Ä‘Ã¡nh giÃ¡

#### 3.1.1. Äiá»ƒm máº¡nh cá»§a há»‡ thá»‘ng

**Vá» máº·t ká»¹ thuáº­t:**
- Kiáº¿n trÃºc end-to-end hoÃ n chá»‰nh tá»« data ingestion Ä‘áº¿n user interface
- TÃ­ch há»£p thÃ nh cÃ´ng cÃ¡c cÃ´ng nghá»‡ Big Data hiá»‡n Ä‘áº¡i (Hadoop, Spark, Elasticsearch)
- Kháº£ nÄƒng má»Ÿ rá»™ng vÃ  fault tolerance vá»›i 3-node cluster
- Real-time processing capabilities vá»›i streaming data
- Sá»­ dá»¥ng containerization vÃ  orchestration vá»›i VirtualBox

**Vá» máº·t xá»­ lÃ½ dá»¯ liá»‡u:**
- Pipeline xá»­ lÃ½ dá»¯ liá»‡u hoÃ n chá»‰nh vá»›i data cleaning vÃ  validation
- Machine learning models vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao (85% cho salary prediction)
- Feature engineering vÃ  preprocessing cháº¥t lÆ°á»£ng
- Business intelligence thá»±c táº¿ vÃ  cÃ³ giÃ¡ trá»‹ á»©ng dá»¥ng

**Vá» máº·t á»©ng dá»¥ng:**
- RESTful API vá»›i 10+ endpoints phá»¥c vá»¥ external applications
- Kibana dashboards trá»±c quan vá»›i 50+ visualizations
- Web UI demo Ä‘áº§y Ä‘á»§ tÃ­nh nÄƒng
- HÆ°á»›ng dáº«n triá»ƒn khai chi tiáº¿t vÃ  thá»±c táº¿

**Vá» máº·t triá»ƒn khai:**
- HÆ°á»›ng dáº«n cÃ i Ä‘áº·t step-by-step cho mÃ´i trÆ°á»ng production
- Snapshot system Ä‘á»ƒ backup vÃ  restore
- Monitoring vÃ  troubleshooting guides
- Scalable architecture cho future growth

#### 3.1.2. Äiá»ƒm háº¡n cháº¿

**Vá» máº·t ká»¹ thuáº­t:**
- Phá»¥ thuá»™c vÃ o cáº¥u trÃºc website cá»§a cÃ¡c trang tuyá»ƒn dá»¥ng (cÃ³ thá»ƒ thay Ä‘á»•i)
- YÃªu cáº§u tÃ i nguyÃªn há»‡ thá»‘ng lá»›n (48 CPU cores, 128GB RAM cho host)
- Äá»™ phá»©c táº¡p trong deployment vÃ  maintenance
- KhÃ³ scale lÃªn cluster lá»›n hÆ¡n trong mÃ´i trÆ°á»ng production

**Vá» máº·t dá»¯ liá»‡u:**
- Cháº¥t lÆ°á»£ng dá»¯ liá»‡u phá»¥ thuá»™c hoÃ n toÃ n vÃ o nguá»“n thu tháº­p
- Xá»­ lÃ½ ngÃ´n ngá»¯ tiáº¿ng Viá»‡t cÃ²n háº¡n cháº¿ (cáº§n cáº£i thiá»‡n analyzer)
- Thiáº¿u dá»¯ liá»‡u lá»‹ch sá»­ dÃ i háº¡n Ä‘á»ƒ training models tá»‘t hÆ¡n
- Dá»¯ liá»‡u cÃ³ thá»ƒ bá»‹ outdated nhanh chÃ³ng trong thá»‹ trÆ°á»ng lao Ä‘á»™ng

**Vá» máº·t mÃ´ hÃ¬nh:**
- Äá»™ chÃ­nh xÃ¡c cá»§a ML models cáº§n cáº£i thiá»‡n thÃªm (RMSE ~2M VND)
- Cold start problem cho user má»›i (khÃ´ng cÃ³ historical data)
- Interpretability cá»§a má»™t sá»‘ models chÆ°a cao
- ChÆ°a cÃ³ A/B testing Ä‘á»ƒ validate model performance

**Vá» máº·t infrastructure:**
- Single point of failure trong kiáº¿n trÃºc hiá»‡n táº¡i
- ChÆ°a cÃ³ automated backup vÃ  disaster recovery
- Monitoring system cÃ²n cÆ¡ báº£n
- Security hardening chÆ°a Ä‘áº§y Ä‘á»§

### 3.2. HÆ°á»›ng phÃ¡t triá»ƒn

#### 3.2.1. NÃ¢ng cao cháº¥t lÆ°á»£ng dá»¯ liá»‡u

**Má»Ÿ rá»™ng nguá»“n dá»¯ liá»‡u:**
- Vieclam24h, VietnamWorks, Indeed, JobStreet, LinkedIn Jobs
- TÃ­ch há»£p data enrichment tá»« Glassdoor vÃ  Company reviews
- Thu tháº­p dá»¯ liá»‡u real-time vá»›i Kafka streaming
- Sá»­ dá»¥ng APIs chÃ­nh thá»©c thay vÃ¬ web scraping

**Cáº£i thiá»‡n xá»­ lÃ½ ngÃ´n ngá»¯:**
- Sá»­ dá»¥ng ViTokenizer vÃ  PhoBERT cho xá»­ lÃ½ tiáº¿ng Viá»‡t
- Named Entity Recognition cho company names vÃ  skills
- Sentiment analysis cho job descriptions
- Text classification tá»± Ä‘á»™ng cho job categories

**Quality assurance:**
- Automated data validation pipelines
- Duplicate detection algorithms
- Outlier detection vÃ  data cleansing
- Data lineage tracking

#### 3.2.2. Cáº£i thiá»‡n mÃ´ hÃ¬nh AI/ML

**Deep Learning approaches:**
- BERT vÃ  Transformer models cho text understanding
- Computer vision cho resume parsing
- Recommendation systems vá»›i collaborative filtering
- Time series forecasting vá»›i LSTM vÃ  NeuralProphet

**Model enhancement:**
- Ensemble learning (Voting, Stacking, Bagging)
- Hyperparameter optimization
- Cross-validation vÃ  model validation
- Model interpretability vá»›i SHAP vÃ  LIME

**Real-time ML:**
- Online learning cho model updates
- Streaming ML vá»›i Apache Kafka + Spark Streaming
- Model serving vá»›i TensorFlow Serving
- A/B testing framework

#### 3.2.3. Má»Ÿ rá»™ng tÃ­nh nÄƒng

**Mobile application:**
- React Native hoáº·c Flutter app
- Job search vÃ  application features
- Career guidance chatbot
- Push notifications cho job alerts

**Advanced analytics:**
- Predictive career paths
- Skills gap analysis cÃ¡ nhÃ¢n hÃ³a
- Company insights vÃ  competitor analysis
- Industry trend forecasting

**Social features:**
- Job seeker profiles vÃ  networking
- Company pages vÃ  employer branding
- Reviews vÃ  ratings system
- Community forums

#### 3.2.4. Infrastructure improvements

**Cloud migration:**
- AWS EMR, Google Dataproc, Azure HDInsight
- Serverless architecture vá»›i AWS Lambda
- Auto-scaling groups vÃ  load balancers
- Multi-region deployment cho high availability

**Containerization:**
- Docker containers cho táº¥t cáº£ services
- Kubernetes orchestration
- Helm charts cho deployment
- CI/CD pipelines vá»›i GitLab/GitHub Actions

**Monitoring vÃ  observability:**
- Prometheus vÃ  Grafana cho metrics
- ELK stack cho centralized logging
- Distributed tracing vá»›i Jaeger
- Alert system vá»›i PagerDuty/Slack

**Security vÃ  compliance:**
- Data encryption at rest vÃ  in transit
- GDPR vÃ  PDPA compliance
- Role-based access control
- Security audits vÃ  penetration testing

#### 3.2.5. Business development

**B2B solutions:**
- Premium APIs cho doanh nghiá»‡p
- Custom analytics dashboards
- White-label solutions
- Integration vá»›i HR systems (SAP, Workday)

**Partnerships:**
- Äáº¡i há»c vÃ  trÆ°á»ng Ä‘Ã o táº¡o
- Career counseling services
- Recruitment agencies
- Government employment services

**Monetization strategies:**
- Subscription-based model
- Premium reports vÃ  insights
- Job posting platform
- Advertising vÃ  sponsored content

**International expansion:**
- Multi-language support (English, Chinese, etc.)
- Regional job markets
- Cross-border job matching
- Global company database

### 3.3. Káº¿t luáº­n

Há»‡ thá»‘ng Smart Job Market Intelligence System Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai thÃ nh cÃ´ng vá»›i kiáº¿n trÃºc Big Data hoÃ n chá»‰nh, tÃ­ch há»£p cÃ¡c cÃ´ng nghá»‡ tiÃªn tiáº¿n nhÆ° Hadoop, Spark, Elasticsearch vÃ  Kibana. Dá»± Ã¡n khÃ´ng chá»‰ Ä‘Ã¡p á»©ng Ä‘Æ°á»£c yÃªu cáº§u bÃ i táº­p lá»›n mÃ  cÃ²n táº¡o ra má»™t ná»n táº£ng phÃ¢n tÃ­ch thá»‹ trÆ°á»ng lao Ä‘á»™ng cÃ³ giÃ¡ trá»‹ thá»±c tiá»…n.

**ThÃ nh tá»±u Ä‘áº¡t Ä‘Æ°á»£c:**

- **Thu tháº­p dá»¯ liá»‡u:** 2,000+ jobs tá»« TopCV vá»›i pipeline automated
- **Xá»­ lÃ½ dá»¯ liá»‡u:** Spark processing vá»›i ML models (accuracy 85%+)
- **LÆ°u trá»¯:** HDFS + Elasticsearch cluster vá»›i 3 nodes
- **Trá»±c quan hÃ³a:** Kibana dashboards vá»›i 50+ visualizations
- **API:** Flask REST API vá»›i 10+ endpoints
- **Triá»ƒn khai:** HÆ°á»›ng dáº«n cÃ i Ä‘áº·t hoÃ n chá»‰nh cho production

**Táº§m nhÃ¬n tÆ°Æ¡ng lai:**

Há»‡ thá»‘ng hÆ°á»›ng tá»›i trá»Ÿ thÃ nh ná»n táº£ng phÃ¢n tÃ­ch thá»‹ trÆ°á»ng lao Ä‘á»™ng hÃ ng Ä‘áº§u Viá»‡t Nam, má»Ÿ rá»™ng sang ÄÃ´ng Nam Ã vÃ  cung cáº¥p giáº£i phÃ¡p toÃ n diá»‡n cho cáº£ ngÆ°á»i lao Ä‘á»™ng vÃ  doanh nghiá»‡p. Vá»›i viá»‡c Ã¡p dá»¥ng cÃ¡c cÃ´ng nghá»‡ má»›i nhÆ° AI/ML, cloud computing vÃ  real-time analytics, há»‡ thá»‘ng sáº½ tiáº¿p tá»¥c phÃ¡t triá»ƒn Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u ngÃ y cÃ ng cao cá»§a thá»‹ trÆ°á»ng lao Ä‘á»™ng sá»‘ hÃ³a.

---

## PHá»¤ Lá»¤C: THÃ€NH VIÃŠN NHÃ“M

**GVHD:** TS. VÃµ Äá»©c Quang

**SVTH:**
- Nguyá»…n VÄƒn ChÆ°Æ¡ng, 225748010110032 (NT)
- Pháº¡m Quang Chiáº¿n, 225748010110042
- Nguyá»…n Tháº¿ CÃ´ng, 225748010110037
- Nguyá»…n Quang Ãnh, 225748010110021
- Pháº¡m Duy ThÃ¡i, 225748010110037
- Nguyá»…n Kháº¯c QuÃ¢n, 225748010110037

**NgÃ y hoÃ n thÃ nh:** 01/2026

---

## PHá»¤ Lá»¤C A: Káº¾T QUáº¢ THá»°C NGHIá»†M

### A.1. Káº¿t quáº£ thu tháº­p dá»¯ liá»‡u

**Thá»i gian thu tháº­p:** 7 ngÃ y (01/12/2024 - 07/12/2024)

| Nguá»“n dá»¯ liá»‡u | Sá»‘ lÆ°á»£ng báº£n tin | Tá»· lá»‡ thÃ nh cÃ´ng | Thá»i gian trung bÃ¬nh |
|---------------|------------------|------------------|----------------------|
| **TopCV** | 2,456 | 94.2% | 2.3 giÃ¢y/báº£n tin |
| **VietnamWorks** | 1,823 | 87.8% | 3.1 giÃ¢y/báº£n tin |
| **Vieclam24h** | 1,567 | 91.5% | 2.8 giÃ¢y/báº£n tin |
| **Tá»•ng cá»™ng** | **5,846** | **91.2%** | **2.7 giÃ¢y/báº£n tin** |

### A.2. Hiá»‡u suáº¥t há»‡ thá»‘ng

#### A.2.1. Hiá»‡u suáº¥t Hadoop HDFS

| Metric | GiÃ¡ trá»‹ | ÄÃ¡nh giÃ¡ |
|--------|---------|----------|
| **Throughput ghi** | 85 MB/s | Tá»‘t |
| **Throughput Ä‘á»c** | 120 MB/s | Xuáº¥t sáº¯c |
| **Replication time** | 45 giÃ¢y/block | Cháº¥p nháº­n Ä‘Æ°á»£c |
| **Data durability** | 99.99% | Ráº¥t tá»‘t |

#### A.2.2. Hiá»‡u suáº¥t Apache Spark

| Job Type | Thá»i gian thá»±c hiá»‡n | CPU Usage | Memory Usage |
|----------|---------------------|-----------|--------------|
| **Data Cleaning** | 8.5 phÃºt | 65% | 4.2 GB |
| **Feature Engineering** | 12.3 phÃºt | 78% | 6.8 GB |
| **ML Training** | 25.7 phÃºt | 85% | 8.5 GB |
| **Batch Processing** | 15.2 phÃºt | 72% | 5.9 GB |

### A.3. Äá»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh Machine Learning

#### A.3.1. MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n lÆ°Æ¡ng (Salary Prediction)

| Thuáº­t toÃ¡n | RMSE (VNÄ) | RÂ² Score | MAE (VNÄ) | Accuracy |
|------------|------------|----------|-----------|----------|
| **Random Forest** | 1,250,000 | 0.87 | 950,000 | 87.3% |
| **Linear Regression** | 1,850,000 | 0.72 | 1,420,000 | 74.1% |
| **Gradient Boosting** | 1,050,000 | 0.91 | 780,000 | 91.8% |

#### A.3.2. MÃ´ hÃ¬nh phÃ¢n loáº¡i cÃ´ng viá»‡c (Job Classification)

| Metric | Precision | Recall | F1-Score | Accuracy |
|--------|-----------|--------|----------|----------|
| **IT/Software** | 0.92 | 0.89 | 0.91 | - |
| **Marketing** | 0.85 | 0.87 | 0.86 | - |
| **Finance** | 0.88 | 0.84 | 0.86 | - |
| **HR** | 0.79 | 0.82 | 0.81 | - |
| **Overall** | 0.86 | 0.86 | 0.86 | **89.2%** |

### A.4. Hiá»‡u suáº¥t API vÃ  Dashboard

#### A.4.1. API Performance

| Endpoint | Response Time (ms) | Throughput (req/s) | Error Rate |
|----------|-------------------|-------------------|------------|
| `/api/jobs` | 245 | 45.2 | 0.1% |
| `/api/jobs/{id}` | 89 | 120.5 | 0.05% |
| `/api/predict-salary` | 1250 | 12.8 | 0.3% |
| `/api/skill-demand` | 567 | 28.9 | 0.2% |
| `/api/trends` | 723 | 22.1 | 0.1% |

#### A.4.2. Kibana Dashboard Performance

| Dashboard | Load Time (s) | Memory Usage (MB) | CPU Usage (%) |
|-----------|---------------|------------------|---------------|
| **Overview** | 2.3 | 245 | 12.5 |
| **Skills Analysis** | 3.1 | 312 | 15.8 |
| **Salary Insights** | 2.8 | 289 | 14.2 |
| **Geographic View** | 4.2 | 356 | 18.9 |
| **Trends Dashboard** | 3.7 | 334 | 16.7 |

### A.5. Káº¿t quáº£ phÃ¢n tÃ­ch thá»‹ trÆ°á»ng

#### A.5.1. Top 10 ká»¹ nÄƒng Ä‘Æ°á»£c yÃªu cáº§u nhiá»u nháº¥t

| STT | Ká»¹ nÄƒng | Sá»‘ lÆ°á»£ng | Tá»· lá»‡ (%) |
|-----|---------|----------|-----------|
| 1 | Python | 1,245 | 21.3 |
| 2 | SQL | 987 | 16.9 |
| 3 | Java | 756 | 12.9 |
| 4 | JavaScript | 634 | 10.8 |
| 5 | AWS/Azure | 523 | 8.9 |
| 6 | Machine Learning | 445 | 7.6 |
| 7 | Docker/Kubernetes | 389 | 6.7 |
| 8 | React.js | 345 | 5.9 |
| 9 | Git | 298 | 5.1 |
| 10 | Linux | 287 | 4.9 |

#### A.5.2. PhÃ¢n bá»‘ lÆ°Æ¡ng theo ngÃ nh nghá»

| NgÃ nh nghá» | LÆ°Æ¡ng trung bÃ¬nh (VNÄ/thÃ¡ng) | Sá»‘ lÆ°á»£ng jobs |
|------------|-----------------------------|---------------|
| **IT/Software** | 18,500,000 | 2,145 |
| **Data Science/AI** | 22,300,000 | 456 |
| **DevOps/Cloud** | 20,800,000 | 387 |
| **Marketing** | 12,500,000 | 823 |
| **Finance** | 15,200,000 | 634 |
| **HR** | 11,800,000 | 289 |
| **Design** | 13,700,000 | 456 |

#### A.5.3. Xu hÆ°á»›ng tuyá»ƒn dá»¥ng theo thá»i gian

**ThÃ¡ng 12/2024:**
- Tá»•ng sá»‘ jobs: 1,234
- TÄƒng trÆ°á»Ÿng: +15.7% so vá»›i thÃ¡ng trÆ°á»›c
- NgÃ nh hot nháº¥t: IT (+23.4%)
- Ká»¹ nÄƒng tÄƒng máº¡nh: Python (+31.2%), AI/ML (+45.8%)

---

## PHá»¤ Lá»¤C B: MÃƒ NGUá»’N CHÃNH

### B.1. Script thu tháº­p dá»¯ liá»‡u (crawler.py)

```python
#!/usr/bin/env python3
"""
Job Market Data Crawler - Thu tháº­p dá»¯ liá»‡u tá»« TopCV
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import logging

class TopCVCrawler:
    def __init__(self):
        self.base_url = "https://www.topcv.vn"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        })

        # Cáº¥u hÃ¬nh logging
        logging.basicConfig(
            filename='crawler.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def crawl_jobs(self, max_pages=10):
        """Thu tháº­p dá»¯ liá»‡u jobs tá»« TopCV"""
        jobs = []

        for page in range(1, max_pages + 1):
            try:
                url = f"{self.base_url}/tim-viec-lam-it-phan-mem?page={page}"
                self.logger.info(f"Äang crawl trang {page}: {url}")

                response = self.session.get(url, timeout=30)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, 'html.parser')
                job_cards = soup.find_all('div', class_='job-item')

                for card in job_cards:
                    job_data = self.extract_job_data(card)
                    if job_data:
                        jobs.append(job_data)

                self.logger.info(f"Thu tháº­p Ä‘Æ°á»£c {len(job_cards)} jobs tá»« trang {page}")
                time.sleep(2)  # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block

            except Exception as e:
                self.logger.error(f"Lá»—i khi crawl trang {page}: {str(e)}")
                continue

        return jobs

    def extract_job_data(self, job_card):
        """TrÃ­ch xuáº¥t thÃ´ng tin tá»« job card"""
        try:
            # Láº¥y thÃ´ng tin cÆ¡ báº£n
            title_elem = job_card.find('h3', class_='title')
            company_elem = job_card.find('a', class_='company')
            salary_elem = job_card.find('div', class_='salary')
            location_elem = job_card.find('div', class_='location')

            if not title_elem or not company_elem:
                return None

            # Parse salary
            salary_text = salary_elem.text.strip() if salary_elem else "ThÆ°Æ¡ng lÆ°á»£ng"
            salary_min, salary_max = self.parse_salary(salary_text)

            job_data = {
                'job_id': f"topcv_{int(time.time())}_{hash(title_elem.text)}",
                'title': title_elem.text.strip(),
                'company': company_elem.text.strip(),
                'location': location_elem.text.strip() if location_elem else "Unknown",
                'salary_min': salary_min,
                'salary_max': salary_max,
                'salary_text': salary_text,
                'source': 'topcv',
                'crawled_at': datetime.now().isoformat(),
                'url': self.base_url + title_elem.find('a')['href'] if title_elem.find('a') else ""
            }

            return job_data

        except Exception as e:
            self.logger.error(f"Lá»—i extract job data: {str(e)}")
            return None

    def parse_salary(self, salary_text):
        """Parse chuá»—i lÆ°Æ¡ng thÃ nh sá»‘"""
        try:
            if "ThÆ°Æ¡ng lÆ°á»£ng" in salary_text or "Thoáº£ thuáº­n" in salary_text:
                return None, None

            # Loáº¡i bá» kÃ½ tá»± khÃ´ng pháº£i sá»‘ vÃ  tÃ¡ch khoáº£ng
            salary_text = salary_text.replace('VNÄ', '').replace('Ä‘', '').strip()

            if '-' in salary_text:
                parts = salary_text.split('-')
                min_salary = self.extract_number(parts[0])
                max_salary = self.extract_number(parts[1])
                return min_salary, max_salary
            else:
                # LÆ°Æ¡ng cá»‘ Ä‘á»‹nh
                salary = self.extract_number(salary_text)
                return salary, salary

        except Exception as e:
            self.logger.error(f"Lá»—i parse salary '{salary_text}': {str(e)}")
            return None, None

    def extract_number(self, text):
        """TrÃ­ch xuáº¥t sá»‘ tá»« chuá»—i"""
        import re
        numbers = re.findall(r'\d+', text.replace('.', '').replace(',', ''))
        if numbers:
            return int(''.join(numbers))
        return None

    def save_to_json(self, jobs, filename=None):
        """LÆ°u dá»¯ liá»‡u vÃ o file JSON"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'jobs_{timestamp}.json'

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ÄÃ£ lÆ°u {len(jobs)} jobs vÃ o {filename}")
        return filename

if __name__ == "__main__":
    crawler = TopCVCrawler()
    jobs = crawler.crawl_jobs(max_pages=5)
    filename = crawler.save_to_json(jobs)
    print(f"HoÃ n thÃ nh! Thu tháº­p Ä‘Æ°á»£c {len(jobs)} jobs, lÆ°u vÃ o {filename}")
```

### B.2. Script xá»­ lÃ½ dá»¯ liá»‡u Spark (spark_processor.py)

```python
#!/usr/bin/env python3
"""
Spark Job Data Processor - Xá»­ lÃ½ dá»¯ liá»‡u vá»›i Spark ML
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline
import logging

class SparkJobProcessor:
    def __init__(self):
        # Cáº¥u hÃ¬nh logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

        # Khá»Ÿi táº¡o Spark session
        self.spark = SparkSession.builder \
            .appName("JobMarketProcessor") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()

        self.logger.info("Spark session initialized")

    def load_data(self, input_path):
        """Load dá»¯ liá»‡u tá»« HDFS"""
        try:
            df = self.spark.read.json(input_path)
            self.logger.info(f"Loaded {df.count()} records from {input_path}")
            return df
        except Exception as e:
            self.logger.error(f"Error loading data: {e}")
            return None

    def clean_data(self, df):
        """LÃ m sáº¡ch dá»¯ liá»‡u"""
        self.logger.info("Starting data cleaning...")

        # Loáº¡i bá» records null
        df_clean = df.dropna(subset=['title', 'company'])

        # Chuáº©n hÃ³a text
        df_clean = df_clean.withColumn('title_clean',
            regexp_replace('title', '[^a-zA-Z0-9\\sÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘Ã€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä]', ''))

        # Xá»­ lÃ½ salary
        df_clean = df_clean.withColumn('salary_avg',
            when(col('salary_min').isNotNull() & col('salary_max').isNotNull(),
                 (col('salary_min') + col('salary_max')) / 2)
            .otherwise(col('salary_min')))

        # Parse experience tá»« title vÃ  description
        df_clean = df_clean.withColumn('experience_years',
            when(col('title').contains('Senior') | col('title').contains('Lead'), 5)
            .when(col('title').contains('Mid') | col('title').contains('Middle'), 3)
            .when(col('title').contains('Junior') | col('title').contains('Fresher'), 1)
            .otherwise(2))

        self.logger.info(f"Data cleaning completed: {df_clean.count()} records")
        return df_clean

    def feature_engineering(self, df):
        """Táº¡o features cho ML"""
        self.logger.info("Starting feature engineering...")

        # Index categorical variables
        indexers = [
            StringIndexer(inputCol='location', outputCol='location_index', handleInvalid='keep'),
            StringIndexer(inputCol='company', outputCol='company_index', handleInvalid='keep')
        ]

        # Táº¡o pipeline cho indexing
        pipeline = Pipeline(stages=indexers)
        df_indexed = pipeline.fit(df).transform(df)

        # Táº¡o feature vector
        assembler = VectorAssembler(
            inputCols=['location_index', 'company_index', 'experience_years'],
            outputCol='features',
            handleInvalid='keep'
        )

        df_featured = assembler.transform(df_indexed)

        # Scale features
        scaler = StandardScaler(inputCol='features', outputCol='scaled_features')
        df_featured = scaler.fit(df_featured).transform(df_featured)

        self.logger.info("Feature engineering completed")
        return df_featured

    def train_salary_model(self, df):
        """Train mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n lÆ°Æ¡ng"""
        self.logger.info("Training salary prediction model...")

        # Lá»c dá»¯ liá»‡u cÃ³ salary
        df_salary = df.filter(col('salary_avg').isNotNull())

        if df_salary.count() == 0:
            self.logger.warning("No salary data available for training")
            return None

        # Chia train/test
        train_data, test_data = df_salary.randomSplit([0.8, 0.2], seed=42)

        # Train model
        rf = RandomForestRegressor(
            featuresCol='scaled_features',
            labelCol='salary_avg',
            numTrees=100,
            maxDepth=10,
            seed=42
        )

        model = rf.fit(train_data)

        # Evaluate
        predictions = model.transform(test_data)
        evaluator = RegressionEvaluator(
            labelCol='salary_avg',
            predictionCol='prediction',
            metricName='rmse'
        )

        rmse = evaluator.evaluate(predictions)
        r2 = RegressionEvaluator(
            labelCol='salary_avg',
            predictionCol='prediction',
            metricName='r2'
        ).evaluate(predictions)

        self.logger.info(".2f"
        # Save model
        model.write().overwrite().save('/models/salary_predictor')

        return model

    def train_classification_model(self, df):
        """Train mÃ´ hÃ¬nh phÃ¢n loáº¡i cÃ´ng viá»‡c"""
        self.logger.info("Training job classification model...")

        # Táº¡o labels tá»« title
        df_classified = df.withColumn('job_category',
            when(col('title').contains('Data') | col('title').contains('AI') | col('title').contains('ML'), 'Data Science')
            .when(col('title').contains('DevOps') | col('title').contains('Cloud') | col('title').contains('AWS'), 'DevOps')
            .when(col('title').contains('Frontend') | col('title').contains('React') | col('title').contains('Vue'), 'Frontend')
            .when(col('title').contains('Backend') | col('title').contains('API') | col('title').contains('Server'), 'Backend')
            .when(col('title').contains('Fullstack') | col('title').contains('Full-stack'), 'Fullstack')
            .when(col('title').contains('Mobile') | col('title').contains('iOS') | col('title').contains('Android'), 'Mobile')
            .otherwise('Other')
        )

        # Index label
        indexer = StringIndexer(inputCol='job_category', outputCol='label')
        df_classified = indexer.fit(df_classified).transform(df_classified)

        # Chia train/test
        train_data, test_data = df_classified.randomSplit([0.8, 0.2], seed=42)

        # Train model
        rf = RandomForestClassifier(
            featuresCol='scaled_features',
            labelCol='label',
            numTrees=50,
            maxDepth=8,
            seed=42
        )

        model = rf.fit(train_data)

        # Evaluate
        predictions = model.transform(test_data)
        evaluator = MulticlassClassificationEvaluator(
            labelCol='label',
            predictionCol='prediction',
            metricName='accuracy'
        )

        accuracy = evaluator.evaluate(predictions)
        self.logger.info(".2f"
        # Save model
        model.write().overwrite().save('/models/job_classifier')

        return model

    def save_to_elasticsearch(self, df, index_name='processed_jobs'):
        """LÆ°u dá»¯ liá»‡u vÃ o Elasticsearch"""
        try:
            df.write \
                .format("org.elasticsearch.spark.sql") \
                .option("es.nodes", "master") \
                .option("es.port", "9200") \
                .option("es.resource", index_name) \
                .option("es.mapping.id", "job_id") \
                .mode("overwrite") \
                .save()

            self.logger.info(f"Saved {df.count()} records to Elasticsearch index: {index_name}")

        except Exception as e:
            self.logger.error(f"Error saving to Elasticsearch: {e}")

    def run_pipeline(self, input_path):
        """Cháº¡y toÃ n bá»™ pipeline xá»­ lÃ½"""
        self.logger.info("=== Starting Job Market Data Processing Pipeline ===")

        # Load data
        df = self.load_data(input_path)
        if df is None:
            return

        # Clean data
        df_clean = self.clean_data(df)

        # Feature engineering
        df_featured = self.feature_engineering(df_clean)

        # Train models
        salary_model = self.train_salary_model(df_featured)
        classification_model = self.train_classification_model(df_featured)

        # Apply predictions
        if salary_model:
            df_featured = salary_model.transform(df_featured)

        if classification_model:
            df_featured = classification_model.transform(df_featured)

        # Save to Elasticsearch
        self.save_to_elasticsearch(df_featured)

        self.logger.info("=== Pipeline completed successfully ===")

if __name__ == "__main__":
    import sys

    processor = SparkJobProcessor()

    # Input path tá»« command line hoáº·c default
    input_path = sys.argv[1] if len(sys.argv) > 1 else 'hdfs://master:9000/raw-data/topcv/*.json'

    processor.run_pipeline(input_path)
```

---

## TÃ€I LIá»†U THAM KHáº¢O

1. "Big Data Analytics with Spark" - Mohammed Guller
2. "Hands-On Machine Learning with Scikit-Learn" - AurÃ©lien GÃ©ron
3. "Elasticsearch: The Definitive Guide" - Clinton Gormley
4. "Learning Spark" - Jules S. Damji et al.
5. Apache Spark Documentation - https://spark.apache.org/docs/latest/
6. Elasticsearch Guide - https://www.elastic.co/guide/
7. Kibana Documentation - https://www.elastic.co/kibana
8. "Data Science from Scratch" - Joel Grus
9. Vietnam IT Job Market Report 2024 - TopCV Research
10. "Deep Learning for Coders with fastai" - Jeremy Howard
11. VirtualBox Documentation - https://www.virtualbox.org/
12. Ubuntu Server Guide - https://ubuntu.com/server/docs
13. Hadoop: The Definitive Guide - Tom White
14. Flask Documentation - https://flask.palletsprojects.com/

---

*Háº¿t bÃ¡o cÃ¡o*
